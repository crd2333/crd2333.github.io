#import "/src/components/TypstTemplate/lib.typ": *

#show: project.with(
  title: "操作系统原理与实践",
  lang: "zh",
)

#info()[
  - 感觉 #link("https://note.hobbitqia.cc/OS/")[hobbitqia 的笔记] 比较好
  - 还有 #link("https://note.isshikih.top/cour_note/D3QD_OperatingSystem/")[修佬的笔记]，虽然老师不一样，具体内容上也有一定差别
]

#note()[
  - 这一部分就是内存相关的一大块内容，属于本门课的核心了
]

#counter(heading).update(7)

#let WSSi = math.text("WSSi")
#let WS = math.text("WS")

= Main Memory
- 从这里开始跨入 OS 内存管理的新纪元
- 背景
  - 程序必须被（从磁盘）拿到内存并放置在进程中才能运行
  - *Main memory* and *registers* are only storage that CPU can access directly，其中 rigister access 只用 one CPU clock (or less)，而 main memory 需要 stall
  - Protection of memory is required to ensure correct operation

== Partition evolution
- In the beginning
  - 最开始我们把一个程序加载到物理内存里，只能执行一个 job，如果 job 比物理内存还大就要分治 divide and conquer，需要 partition
  - 后来我们有多个进程要同时进行，也是使用 partition 的方法，不同的分区执行不同的进程
- Partition requirements
  - *Protection* – keep processes from smashing each other
  - *Fast execution* – memory accesses can’t be slowed by protection mechanisms
  - *Fast context switch* – can’t take forever to setup mapping of addresses
- Physical Memory
  - 问题
    + 不能挪，否则需要更新大量的指针
    + 内存空隙，运行到后期巨量碎片无法使用
  - 从而提出（初版的）Logical Memory，由我们自己定义的地址形式。具体翻译为物理地址由硬件实现
- Logical Memory v1
  - *offset within a partition*
  - Base and Limit registers
    - *Base* added to all addresses
    - *Limit* checked on all memory references 每次访问时检查是否超过了 Limit，如果是就说明越界了
    - Loaded by OS at each *context switch*
    - 每个进程有自己的 base 和 limit 寄存器，每次进程切换时，OS 都会将 base 和 limit 寄存器的值更新为当前进程的值（线程不需要，因为线程是共享的地址空间）
    - Partition 怎么做权限管理？跟后面 segment 类似，有个 table 填 `rwx`
    #grid2(
      fig("/public/assets/Courses/OS/2024-11-05-14-38-15.png", width: 70%),
      fig("/public/assets/Courses/OS/2024-11-05-14-42-54.png")
    )
  - Advantages
    + Built-in protection provided by Limit: No physical protection per page or block
    + Fast execution: Addition and limit check 的实现可以挪到硬件上
    + Fast context switch: Need only change base and limit registers
    + No relocation of program addresses at load time: All addresses relative to zero
    + Partition can be suspended and moved at any time
      - Process is unaware of change. 修改 base 即可移动进程，进程是意识不到的。
      - Expensive for large processes. 移动进程需要改 base，还要把旧的内容全部改到新的位置，耗时
  - 接下来我们思考应该问题，partition 应该多大？

== Memory Allocation Strategies
- Fixed partitions
  - 所有 partition 的 size 是固定的，易于实现
  - 但是要切多大？
    - 如果切的太小，可能有大进程无法加载进来（只能 divide and conquer）
    - 如果切的太大，会有*内部碎片*
- Variable partitions
  - 长度不一致，按需划分。即要给一个进程分配空间时，我们找到比他大的最小的 partition，然后把他放进去
  - 有 3 种分配方法
    + first-fit: allocate from the first block that is big enough
    + best-fit: allocate from the smallest block that is big enough
    + worst-fit: allocate from the largest hole
    - *考一个选择题*
  - Problem – *外部碎片 External Fragmentation*: Variable partitions 可以避免内部碎片，但无论如何总是有外部碎片，在 partition 之外的空闲空间太小，无法被任何进程使用，此时我们需要碎片整理
  - 亦或者，我们可以用 Segmentation 机制来把进程分到多个 section

== Segmentation
- 从这里开始，section, or partition, or segmentation 都是一个概念
- 一个程序分成 text、data、stack 等多个区域，每个区域就用一个 partition 来代表它
- Logical Address v2
  - *`<segment-number, offset>`* segment-number 表示属于第几组，offset 表示 segment 内的偏移量
  #fig("/public/assets/Courses/OS/2024-11-05-15-13-12.png", width: 70%)
  - 每个进程有自己的 *Segment register table（段表）*，通过 limit 实现 variable partitions
    #tbl(columns:3,[base],[limit],[权限],[base],[limit],[权限],[...],[...],[...])
    - 可以看到它额外实现了权限控制（但是跟之后的 paging 机制比，这个权限控制很不细粒度）
- 然而 Segmentation 并没有完全解决外部碎片的问题
  - 我们之后将会利用 fixed partitions 的办法来解决 —— Paging

== Address Binding & Memory-Management Unit
- 到这里我们先做个总结
- 在程序的不同阶段，地址有不同的表现方式：
  - source code addresses are usually symbolic. (e.g., variable name)
  - compiler binds symbols to relocatable addresses. (e.g., “14 bytes from beginning of this module”)
  - linker (or loader) binds relocatable addresses to absolute addresses.
- Logical v.s. Physical Address
  - Logical address – generated by the CPU; also referred to as virtual address.
    - CPU 看不到物理地址，只用逻辑地址，需要经过特定的部件转化为物理地址。
  - Physical address – address seen by the memory unit.
    - 内存单元只能理解物理地址，它是无法改变的。
  - 物理地址对应物理地址空间(Physical Address Space)，逻辑地址对应“逻辑地址空间”(Logical Address Space)，实际上并不存在，而是由一个映射所定义
- MMU
  - 我们之前说要把逻辑地址到物理地址的转换放到硬件上实现，从而不影响速度，这就是 MMU(Memory-Management Unit)
  #grid2(
    fig("/public/assets/Courses/OS/2024-11-05-15-29-00.png"),
    fig("/public/assets/Courses/OS/2024-11-05-15-29-05.png")
  )

#note(caption: [takeaway])[
  - Partition evolution
  - Memory partition-fixed and variable
    - *first, best, worst fit*
    - fragmentation: internal / external
  - Segmentation
    - Logical address vs physical address
  - MMU:address translation protection
]

== Paging
- Fixed 和 Variable 划分都是物理连续的分配，Paging 是把所有内存都变成不连续的，这样空闲的内存不管在哪，都可以分配给进程，避免了外部碎片
- Basic methods
  - Divide *physical* address into fixed-sized blocks called *frames*（物理帧号）
    - Keep track of all free frames.
  - Divide *logical* address into blocks of same size called *pages*（虚拟页号）
  - 页和帧是一样大的，Size is power of 2, usually 4KB
  - 为了跑一个 $N$ pages 的程序，需要找到 $N$ 个 free 的 frames 把程序加载上去
  - 把 $N$ 个帧映射到 $N$ 个页，这个存储帧到页的映射的数据结构叫*页表 page table*
- Paging has no *external fragmentation*, but *internal fragmentation*
  - 那为什么我们要从 variable partition 又回到 fixed partition 呢？因为此时内部碎片问题不严重（一个进程被拆成多个 page，只有最后的页才会有碎片），比之前的 partition 要小很多
    - worst case: 1 frame - 1 byte; average internal fragmentation: 1 / 2 frame size
  - page(frame) size
    - 页如果小，碎片少，但是映射更多，页表需要更大的空间
    - 反之页如果大，碎片多但映射更少，页表较小
    - 现在页逐渐变大，因为内存变得 cheap，一点碎片不影响

=== Page Table
- Page table: Stores the logical page to physical frame mapping
- Frame table: 一个 Bitmap，标记哪些 frame 是空闲的
- 页表不存页号（页号用作索引），只存物理帧号
#fig("/public/assets/Courses/OS/2024-11-05-15-41-57.png", width: 60%)
- Logical Address v3
  #fig("/public/assets/Courses/OS/2024-11-05-15-44-39.png", width: 40%)
  - 和之前 Segmentation 机制的 Logical Address 很像，区别在于 fixed partition 和 variable partition
  - *page number (p)*
    - used as an index into a page table
    - page table entry contains the corresponding physical frame number
  - *page offset (d)*
    - offset within the page/frame
    - combined with frame number to get the physical address
- MMU 的变化
  - 首先把 p 拿出来，到页表里读出读出物理帧号，随后和 d 拼接起来就得到了物理地址
  #fig("/public/assets/Courses/OS/2024-11-05-15-48-45.png", width: 60%)

=== Paging Hardware
- 现在我们思考怎么实现页表
- Simple Idea
  - 早期想法：a set of dedicated registers
  - 优势是非常快，但是缺点是寄存器数量有限，无法存储多的页表（如 $32$ bit 地址，$20$ 位作为物理页号，需要 $2^20$ 个页）
- Alternative Way
  - 存储在 main memory 里
  - *page-table base register (PTBR)* 指向页表的起始地址
    - RISC-V 上叫 SATP
    - ARM 上叫 TTBR
    - x86 上叫 CR3
  - *page-table length register (PTLR)* indicates the size of the page table
- 这样每次数据/指令访问需要两次内存访问，第一次把页表读出来，第二次再根据页表去读数据，显然变慢了，如何解决？遇事不决加 cache！如果 hit 了就只用一次内存访问
- *TLB (translation look-aside buffer)* caches the address translation
  - TLB hit: if page number is in the TLB, no need to access the page table.
  - TLB miss: if page number is not in the TLB, need to replace one TLB entry
    - 在 MIPS 上 TLB miss 是由 OS 处理的，但是在 RISC-V 上是由硬件处理的
  - TLB usually use a fast-lookup hardware cache called associative memory
    - 与页表不同的是，TLB 里存储的既有 page number 又有 frame number，通过比较 page number 来找到对应的 frame number（相当于全相联的 cache）
    - Associative memory: 一种支持并行搜索的内存，不由 "addresses" 寻址而是由内容，如果 `page#` 与其中键匹配上，则直接返回 `frame#`(value)
  - TLB is usually small, 64 to 1024 entries. TLB 数量有限，为了覆盖更大的区域，我们也想要把页变得更大。
- 每个进程有自己的页表，所以我们 context switch 时也要切换页表，要把 TLB 清空(TLB must be consistent with page table)
  - Option I: Flush TLB at every context switch, or,
  - Option II: Tag TLB entries with address-space identifier (ASID) that uniquely identifies a process. 通用的全局 entries 不刷掉，把进程独有的 entries 刷掉
- More on TLB Entries
  - Instruction micro TLB
  - Data micro TLB
  - Main TLB
    - A 4-way, set-associative, 1024 entry cache which stores VA to PA mappings for 4KB, 16KB, and 64KB page sizes.
    - A 2-way, set-associative, 128 entry cache which stores VA to PA mappings for 1MB, 2MB, 16MB, 32MB, 512MB, and 1GB block sizes.
- More on TLB Match Process
  - 不要求掌握
- 现在 MMU 的变化
  - 多了个 TLB
  #fig("/public/assets/Courses/OS/2024-11-06-16-36-25.png",width: 60%)
- Effective Access Time（要会算）
  #fig("/public/assets/Courses/OS/2024-11-06-16-39-17.png",width: 70%)
- 题外话：Segmentation 和 Paging 两个机制其实是差不多时间(1961 and 1962)发明的，后者更优越但硬件上实现更难，所以更晚被广泛使用

=== Page with Memory Protection and Sharing
- Memory Protection
  - 到目前为止，页表里放了物理帧号。我们可以以页为粒度放上保护的一些权限（如可读、写、执行），这样就可以实现内存保护
    - Each page table entry has a present (aka. valid) bit
      - present: the page has a valid physical frame, thus can be accessed
    - Each page table entry contains some protection bits.
      - 任何违反内存保护的行为导致 kernel 陷入 trap
  - XN: protecting code
    - 把内存分为 code 和 data 区，只有 code 区可以执行。e.g. Intel: XD(execute disable), AMD: EVP (enhanced virus protection), ARM: XN (execute never)
  - PXN: Privileged Execute Never
    - A Permission fault is generated if the processor is executing at EL1(kernel) and attempts to execute an instruction fetched from the corresponding memory region when this PXN bit is 1 (usually user space memory). e.g. Intel: SMEP
  #fig("/public/assets/Courses/OS/2024-11-06-16-45-02.png", width: 60%)
- Page Sharing
  - Paging allows to share memory between processes
    - shared memory can be used for inter-process communication
    - shared libraries
  - 同一程序的多个进程可以使用同一份代码，只要这份代码是 reentrant code(non-self-modifying code: never changes between execution)
  - 共享代码在各个进程中的逻辑地址空间相同，然后每个进程再花较小的空间保存私有代码和数据即可。
  #fig("/public/assets/Courses/OS/2024-11-06-16-56-43.png",width: 50%)

=== Structure of Page Table
- *重要*！！！Page Table 需要物理地址连续(*physically contiguous*)，因为它是由 MMU 去管的，MMU 不知道 logical address 这件事
- 如果只有一级的页表，那么页表所占用的内存将大到不可接受
  - e.g. $32bit$ logical address space and $4KB$ page size. page table 需要 $2^32 \/ 2^12=1M$ 个 entries。如果每个 entry 是 $4bytes$，那一共就是 $4MB$
  - 我们需要有方法压缩页表
    - 考虑到 Logical addresses have holes
    - Break up the logical address space into multiple-level(Hierarchical) of page tables. e.g. two-level page table
    - First-level page table contains the `frame#` for second-level page tables.
  #fig("/public/assets/Courses/OS/2024-11-06-17-12-01.png", width: 60%)
- 最极端的例子($32$ bit, $4$ bytes for each entry)
  #fig("/public/assets/Courses/OS/2024-11-06-17-19-59.png", width: 60%)
  - 页表为什么可以省内存？如果次级页表对应的页都没有被使用，就不需要分配这个页表
    - 关于页表的空间节省计算，可以参考 #link("https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter4/3sv39-implementation-1.html#id6")[rCore-Tutorial-Book]
  - 最坏情况下，如果只访问第一个页和最后一页，那么只用一级页表需要 $1K$ 个页用来放页表（这个页表有 $2^20$ 个条目），但是对于二级页表就只需要 $3$ 个页表（$1$ 个一级和 $2$ 个二级页表），即 $3$ 个页来放页表。内存占用 $4M -> 12K$
- Logical Address v4
  - 对二级页表: `<PGD, PTE, offset>`
    - a page directory number (1st level page table), a page table number (2nd level page table), and a page offset
    #fig("/public/assets/Courses/OS/2024-11-06-17-22-03.png", width: 60%)
  - 多级页表每一级的命名规则是，固定最小的是 PTE，最大的是 PGD；如果是更多级页表，PTE 之上是 PMD，再之上是 PUD，再上已经没有名字了所以取了个 P4D
  - 一个比较生草的问题是页表里以及 PTBR 存的是 logical address 还是 physical address，答案肯定是后者，因为我们本来就是在做 LA $->$ PA 的转译，要还是 LA 就“鸡生蛋蛋生鸡”了。同样的理由，page table 内存储的物理地址应该是连续的
  - 另外这里经常出 *page size* 和 *entry size* 变化后的分区大小问题
    - 如果 page size 变大，offset 需要变大，Page Table 能容纳的 entries 也变多；如果 entry size 变大……
- 例如，$64$ bit 下，每个页表 entry size 变为 $8B$，一个页可以放 $2^12\/2^3=512$ entries
  - $64$ bit 能索引的地址太大了，一般都用不完
    - AMD-64 supports $48$ bits; ARM64 supports $39$ bits, $48$ bits
    - 对 $39=9+9+9+12$ bits，有 $3$ 级页表，能索引 $1$ GB
    - 对 $48=9+9+9+9+12$ bits，有 $4$ 级页表，能索引 $512$ GB
    - 对 $57=9+9+9+9+9+12$ bits，有 $5$ 级页表，已经能索引 $128$ PB 了
  #tbl(
    columns: 7,
    [#h(8pt)],[9],[9],[9],[9],[9],[12],
  )

=== Other Page Tables
- 下面我们介绍其它 Page Table
- *Hashed Page Tables*
  - 在地址空间足够大且分布足够稀疏的时候有奇效（因为如果地址空间太大，用 $5$ 级页表最坏情况下要做足足 $5$ 次访存）
  - In hashed page table, virtual `page#` is hashed into a `frame#`
  - 哈希页表的每一个条目除了 page number 和 frame number 以外，还有一个指向有同一哈希值的下一个页表项的指针
  #fig("/public/assets/Courses/OS/2024-11-12-14-40-23.png", width: 60%)
- *Inverted Page Tables*
  - 动机：LA 一般远大于 PA，造成需要 index 的项很多，inverted page table 的想法是去索引 physical frame 而不是 logical pages；另外一个不太重要的原因是，历史上由于 linux 发展较慢，$32 bits$ 只能支持 $4GB$，比 $8GB$ 内存要小
  - 每个 physical frame 对应一个 entry $-->$ 整个 Page Table 占用的内存是固定的！每个 entry 储存 pid 和 page number（对比之前 hierarchical page table 存 frame number，而且它不存 pid 因为每个进程独享自己的 page table），也就是说，Inverted page tables 索引 PA 而不是 LA
  - 现在寻址时，为了 translate LA 到 PA，找到对应有这个 page number 的 entry，不能像原版 page table 那样把 frame number 当做 index 直接找了，必须遍历整个页表找到对应的 pid 和 page number，其在页表中所处的位置即为 frame number
    - 这可以用 TLB 来加速，但是 TLB miss 时代价是很大的
  - 而且这样不能共享内存，因为一个物理帧只能映射到一个页（除非你把每个 entry 做成一个链表进去，也是一种实现）；对比原版 page table，shared memory 只需要两个进程的 page table 指向同一个物理帧号即可
  #fig("/public/assets/Courses/OS/2024-11-12-14-35-28.png", width: 60%)

== Swapping
- 我们前面说 Paging 机制对内存消耗还是比较大的，假如物理内存用完了，能不能把一部分进程放到磁盘上呢？
  - swap out: 用 disk 备份内存，就把 frame 的值交换到 disk 上，然后把 frame 释放出来
  - swap in: 当进程要执行的时候，再把 frame 从 disk 读回来。换回来时不需要相同的物理地址，但是逻辑地址要是一样的
  - 显然这个过程是很慢的，因此当进程在 swap 的时候，会被丢到硬盘的 waiting queue 里
  #fig("/public/assets/Courses/OS/2024-11-12-14-42-26.png", width: 60%)
- Swapping with Paging
  - 为了减轻负担，我们并不是把整个进程塞到 disk，而是部分 page
  - 这样，我们在 load 进程的 disk 部分的同时，进程还在 main memory 的部分可以先执行
    - 换句话说，paging 机制让我们拥有了 partially executing 一个进程的能力
  #fig("/public/assets/Courses/OS/2024-11-12-14-42-38.png", width: 60%)

== Example: Intel 32- and 64-bit Architectures
- Intel IA-32 支持 Segmentation 和 Paging
- Intel 32 bit 提出 Physical Address Extension(PAE) 来支持 $4GB$ 以上寻址
- 这部分感觉应该不用太详细了解

#hline()
#note(caption: "Takeaway")[
  - Partition evolution
  - Contiguous allocation
    - Fixed, variable
      - first, best, worst fit
      - fragmentation: internal/ external
    - Segmentation
      - Logical address v.s. physical address
  - Fragmentation
    - Internal， external
  - MMU: address translation + protection
  - Paging
    - Page table
      - Hierarchical, hashed page table, inverted
      - Two-level, three-level, four-level
      - For 32 bits and 64 bits architectures
]
#note(caption: "Page table quiz（看看考试是怎么考的）")[
  - In $32 bit$ architecture, $4KB$ page
  + for 1-level page table, how large is the whole page table?
    - $4KB$
  + for 2-level page table, how large is the whole page table?
    + How large for the 1st level PGT?
      - $4KB$
    + How large for the 2nd level PGT?
      - $1K times 4KB = 4MB$
  + Why can 2-level PGT save memory?
    - 允许内存不连续 + 可以按需取用（如果次级页表对应的页没有被使用就不需要分配）
  + 2-level page table walk example
    + Page table base register holds `0x0061,9000`
    + Virtual address is `0xf201,5202`
      #tbl(columns:3,[PGD$(10)$],[PTE$(10)$],[offset$(12)$],[968],[21],[514])
      - 在 PTBR 中提取 PGD 的地址，然后加上 index 取 PTE 地址……
    + Page table base register holds `0x1051,4000`
    + Virtual address is `0x2190,7010`
      #tbl(columns:3,[PGD$(10)$],[PTE$(10)$],[offset$(12)$],[134],[263],[16])
      - 在 PTBR 中提取 PGD 的地址，然后加上 index 取 PTE 地址……
      - 题外话：$4KB + 32 bits$ 真的是绝配，对别的 page size、bits 架构就不是这样，如下
  - How about page size is $64KB$
    + What is the virtual address format for 32-bit?
      #tbl(columns:3,[PGD],[PTE],[offset],[2],[14],[16])
    + What is the virtual address format for 64-bit?
      - for $39 bit$ VA —— 只能支持两级页表
      #tbl(columns:4,[PGD],[PMD],[PTE],[offset],[],[10],[13],[16])
      - for $48 bit$ VA —— 可以支持三级页表
      #tbl(columns:4,[PGD],[PMD],[PTE],[offset],[6],[13],[13],[16])
  - 以及要学会画 page table walk 的图和过程
]

= Virtual Memory
== Introduction
- 有了 paging 和 page table 的概念，我们能用来做什么？
- Background: 代码需要在内存中执行，但很少需要或同时使用整个程序
  - unused code: error handling code, unusual routines
  - unused data: large data structures
- *partially-loaded*（在 Swapping 部分已经提到这种思想）
  - 我们可以把还没用到的 code 和 data 延迟加载到内存里，用到时再加载
  - 另一个好处是，program size 可以不受 physical memory size 的限制
- 为了实现部分加载，我们有一个虚拟内存（大致上和逻辑地址类似）的概念，主要靠 Paging 来实现
  - 需要注意的是虚拟地址只是范围，并不能真正的存储数据，数据只能存在物理空间里
#grid(
  columns: 2,
  [
    #fig("/public/assets/Courses/OS/2024-11-13-16-25-55.png")
    - 这样，右图的 user stack 就处在连续的虚拟地址下，但它们经页表映射后的帧并不连续，而且不一定都在内存中
  ],
  fig("/public/assets/Courses/OS/2024-11-19-14-27-31.png", width:50%)
)

== Demand Paging
- *Demand paging*: 一般 OS 采用的方法是，当页被需要的时候(when it is demanded)才被移进来(page in)，demand 的意思是 access(read/write)
  - if page is invalid (error) $-->$ abort the operation
  - if page is valid but not in physical memory $-->$ bring it to physical memory
    - 这就叫 *page fault*
  - 优劣：no unnecessary I/O, less memory needed, slower response, more apps. 简而言之，用时间换取空间
- 三个核心问题
  - Demand paging 和 page fault 的关系？
    - 前者是利用后者实现的
  - What causes page fault？
    - User space program accesses an address
  - Which hardware issues page fault and Who handles page fault?
    - MMU & OS 后面详细展开
- Demand paging 需要硬件支持：
  + page table entries with valid / invalid bit
  + backing storage (usually disks)
  + instruction restart
- 另外这里我们可以思考 Segmentation 能不能实现 demand paging 机制？其实是不太行的，因为它的粒度太大了，就算实现了效果也不好

== Page Fault
- 比如，C 语言调用 `malloc` 的时候，采用的就是 lazy allocation 策略
  - VMA 是 Virtual Memory Area，malloc 调用 `brk()` 只是增大了 VMA 的大小（修改 vm_end），但是并没有真正的分配内存
    - VMA 这个数据结构类似于 OS 的“账本”
  - 只有当我们真正访问这个地址的时候，会触发 page fault，然后找一个空闲帧真正分配内存，并做了映射
  - 那有没有直接 allocate 的呢？`kmalloc` 会直接分配虚拟连续、物理连续的内存，`vmalloc` 会直接分配虚拟连续、物理不连续的内存，多用于内核实现
  #fig("/public/assets/Courses/OS/2024-11-13-16-35-45.png",width:70%)
- *MMU issues page fault*，走到页表最低层的时候发现对应的条目的 valid bit 不为 $1$，说明并没有映射，就触发了 page fault
  - $v$ (valid) $->$ frame mapped, $i$ (invalid) $->$ frame not mapped
- *OS handles page fault* (Linux implementation)
  - Page Fault 出现有两种情况（检测是真的 fault 还是只是空头支票没兑现）
    + 一种是地址本身超过了 VMA 的范围，或者落在 Heap 内但权限不对，这种情况操作系统会杀死进程；
      - 为了判断地址是否落在 VMA 里，Linux 使用了红黑树来加速查找
    + 否则，这个时候 OS 就会分配一个 free frame，然后把这个页映射到这个帧上。但这个时候也分两种情况：
      + *Major*: 这个 page 属于 file-backed(e.g. Data, Text)，它不在内存里面，这时需要先从磁盘读取这个 page，然后映射
      + *Minor*: 这个 page 属于 anonymous(e.g. BSS, Heap, Stack)，它本身就在内存里，这时只需要直接映射即可
  #fig("/public/assets/Courses/OS/2024-11-13-16-44-38.png",width:80%)
  - 具体来说就是
    + MMU 先去 access 这个地址，发现 valid bit 是 $i$，issue page fault
    + OS handle page fault，检查之后发现是合法的，分两种情况
      + Major page fault: 把这个页从磁盘读到内存，然后 reset 页表对应的 valid bit $i --> v$
      + Minor page fault: 找一个 free frame 映射到它，省了 $3,4$ 两步，然后 reset 页表对应的 valid bit $i --> v$
    + 这样之后，重新执行一遍指令，MMU 再重新走一遍这个过程，去 access 这个地址，去 TLB 里找就 miss 了（又一个 fault），这时候把它从页表搬到 TLB 里
    #fig("/public/assets/Courses/OS/2024-11-13-16-54-40.png",width:60%)
    - 这个过程里图中 $4$ 最耗时间，因为要读磁盘。如果跟 schedling 结合，此时会把该进程 sleep，丢到 disk 的 waiting queue 里。等 disk 做完了，触发一个 interrupt，然后 OS 会把这个进程移到 ready queue 里
- How to Get Free Frame
  - OS 为内存维护一个 free-frame list
  - Page fault 发生时，OS 从 free list 里拿一个空闲帧进行分配
  - 为了防止信息泄露，在分配时把帧的所有位都置 $0$ (zero-fill-on-demand)
  - 没有空闲的帧怎么办？之后讲 (page replacement)
- Page Fault with swapper
  - 还是说 page replacement 的 case，要把页换进来(swap in)和换出去(swap out)
  - Lazy swapper: 懒惰执行 swap in，只有需要的时候才真正 swap in
    - the swapper that deals with pages is also called a pager.
  - Pre-Paging: pre-page all or some of pages a process will need, before they are referenced.
    - 空间换时间，减少 page fault 的次数（主要是想少掉 major 的），但是如果 pre page 来的没被用就浪费了

#note(caption: [Stages in Demand Paging – Worse Case])[
  + Trap to the operating system.
  + Save the user registers and process state. (pt_regs)
  + Determine that the interrupt was a page fault.
    - Check that the page reference was legal and determine the location of the page on the disk.
  + Find a free frame
  + Determine the location of the page on the disk, issue a read from the disk to the free frame
    + Wait in a queue for this device until the read request is serviced.
    + Wait for the device seek and/or latency time.
    + Begin the transfer of the page to a free frame.
  + While waiting, allocate the CPU to other process.
  + Receive an interrupt from the disk I/O subsystem. (I/O completed)
    + Determine that the interrupt was from the disk.
    + Mark page fault process ready.
  + Handle page fault: wait for the CPU to be allocated to this process again.
    + Save registers and process state for other process.
    + Context switch to page fault process.
  + Correct the page table and other tables to show page is now in memory.
  + Return to user: restore the user registers, process state, and new page table, and then resume the interrupted instruction.
  - 其实跟之前总结得差不太多，只是再结合 context switch
  #fig("/public/assets/Courses/OS/2024-11-13-17-25-11.png",width:70%)
]

== Demand Paging Optimizations
- 先来分析一下 demand paging 的 overhead
  - page fault rate: $0 =< p =< 1$
  - Effective Access Time(EAT):
    $ (1-p) times "memory access" + p times ("page fault overhead" + "swap page out" + "swap page in" + "instruction restart overhead") $
  #fig("/public/assets/Courses/OS/2024-11-13-17-36-32.png",width:60%)
  - 真实场景下，确实可以让减速比 $=< 10%$，因为有 program locality，而且也不是每个 page fault 都是 major
- Discard
  - 仍旧从 disk 读取(page in)，但是对于部分只是拿来读的数据（比如 Code）或并非 dirty 的数据，我们不需要把它写回 disk（写了也是白写），而是直接丢弃，下次直接从 disk 读取（少一次 I/O）
  - 但下列情况还是需要写回
    - Pages not associated with a file (like stack and heap) – anonymous memory
    - Pages modified in memory but not yet written back to the file system
- Mobile systems
  - 通常不支持 swapping
  - 作为替代，demand paging 从文件系统读取页面并回收 (reclaim) 只读页面(e.g. code)
- Copy-on-Write (COW)
  - 我们之前讲 `fork()` 的时候说过，child 从 parent 完全复制，这是很耗时的，COW 加快了进程创建速度
  - 我们可以让 child 跟 parent 使用 shared pages，只有当父进程或子进程修改了页的内容时，才会真正为修改的页分配内存（copy 并修改）
  - `vfork` syscall optimizes the case that child calls `exec` immediately after `fork`
    - `vfork` 可能很脆弱，它是在 COW 还未实现时发明的
  #fig("/public/assets/Courses/OS/2024-11-13-17-45-30.png", width: 60%)

== Page Replacement
- 没有空闲的物理帧时应该怎么办呢？
  - 我们可以交换出去一整个进程从而释放它的所有帧；
  - 更常见地，我们找到一个当前不在使用的帧，并释放它
  - （听起来像是 frame replacement？但其实 frame 一直在那里，只是 page 变了
- Page replacement: find some page in memory but not really in use, and page it out
  - 与物理地址无关
  #fig("/public/assets/Courses/OS/2024-11-13-17-48-23.png", width: 60%)
- page replacement 是 page fault handler 的一部分
  - 用于选择 victim 的 pocilies 需要精心设计以避免 thrashing
  - 使用 modified (dirty) bit 来减少 swap out 的次数（通常只有
  - 它进一步补充了逻辑和物理内存空间的分隔

=== Page Replacement Mechanism
- Page Fault Handler (with Page Replacement) 为了 page in 一个 page，需要
  + 查找所需页在磁盘上的位置
  + 查找空闲帧：
    + 如果有直接使用
    + 如果没有就用置换算法选择一个 victim frame，并按需将 victim 的内容写回磁盘，更新页表
  + 将所需页读入空闲帧，更新页表
  + 重启引发 trap 的指令
  - 一次 page fault 可能发生 2 次 page I/O，一次 out（可能要把脏页写回）一次 in
  #fig("/public/assets/Courses/OS/2024-11-19-14-44-37.png",width:40%)

=== Page Replacement Algorithms
- 就像 Scheduling 一样，这里我们也需要对 page 研究算法好坏
  - 之后我们也可以思考一下这跟 scheduling 有什么异同
  - 如何评价？用一串 memory reference string，每个数字都是一个页号，给出物理页的数量，看有多少个 page faults（考试必考）
- 比如
  - FIFO, optimal, LRU, LFU, MFU
  - 下面我们考虑 $7,0,1,2,0,3,0,4,2,3,0,3,0,3,2,1,2,0,1,7,0,1$ 这一串数字
- *First-In-First-Out Algorithm (FIFO)*
  - 替换第一个加载进来的 page
  - $15$ page faults with $3$ frames
  - Belady's Anomaly: For FIFO, 增多 frames 不一定减少 page faults
- *Optimal Algorithm*
  - 如果知道后续页号，替换未来最长时间里不会被用的 Page
  - $9$ page faults with $3$ frames
  - 最优算法，但无法预测未来什么时候会访问这些页，用来评价其它算法的好坏
- *Least Recently Used Algorithm (LRU)*
  - 属于 Time-based 方法，替换最近最少被用的
  - 考虑了局部性原理，性能最接近 OPT
  - $12$ page faults with $3$ frames
  - 如何实现？基本上有两种方法
    - counter-based，存时间戳，在每次访问时查找最小的页 (min-heap) 并更新时间戳
    - stack-based，每次访问一个页的时候把它移到栈顶
    - 但这两种方法其实开销都很大，我们有近似的办法，在 PTE 中加了一个 *reference bit*
      - 一开始都设置成 $0$
      - 硬件实现：如果一个 page 被访问，就设置成 $1$
      - 替换时选择 reference bit = 0 (if one exists)
      - 当所有位都设为 $1$ 的时候就只能随机选一个，而且我们无法知道他们的访问顺序
  - LRU 的改进
    - *Additional-Reference-Bits Algorithm*
      - 直觉上，只要我们多设几个 bits，就可以追踪它们的访问顺序。设置 $8$ 个 Bits
      - 每个 time interval (100ms) 将每个 page 的 refernce bits 右移一位，低位抛弃，高位如果被使用就设成 $1$，否则为 $0$
      - 它们记录最近 $8$ 个周期内的使用情况，值越大越最近使用，只要比较大小就可以确定该替换哪个（最小那个）
    - *Second-chance algorithm*
      - 基本算法是 FIFO，添加硬件支持的 reference bit，也叫 *clock replacement algorithm*
      - 给第二次机会，对一个将要被 replaced page，如果它的
        - Reference bit = $0$，直接替换它
        - Reference bit = $1$，把它设置成 $0$，但留在 memory 内（下一次又选到它了才真正换掉它），并且将到达时间更新（因此在所有其它页都被选过一遍之前不会被再次选中）
      - 实现上采用循环队列，用一个指针指向下一次置换的页。当需要一个帧时，指针向前移动并清除引用位，直到找到 $0$
        - 最坏情况下，所有帧都会给二次机会，退化为 FIFO
    - *Enhanced Second-Chance Algorithm*
      - 用 *reference bit* and *modify bit* (if available) 更进一步表征 page 的状态
      - 将引用位和脏位作为有序对：
        - $(0, 0)$ 无引用无修改，置换的最佳页
        - $(0, 1)$ 无引用有修改，置换前需要写回脏页，不是那么好
        - $(1, 0)$ 有引用无修改，很可能会继续用
        - $(1, 1)$ 有引用有修改，很可能会继续用且置换前须要写回脏页
      - 淘汰优先级 $(0,0)-> (0,1)-> (1,0)-> (1,1)$
- *Counting-based Page Replacement*
  - *Least Frequently Used (LFU)*：置换计数最小的，符合直觉。但如果一个页一开始狂用，后来不用了，但因为计数高而不被替换，解决方法是定期右移次数寄存器
  - *Most Frequently Used (MFU)*：置换计数最大的，因为最小次数的页可能刚调进来，还没来得及用
  - 一般是 LFU 相对好一些，但其实都不实用（开销又大效果又差）

#note(caption: "与 scheduling 的异同")[
  #tbl(
    columns: 2,
    [Page Replacement],[Scheduling],
    [FIFO],[FCFS],
    [optimal],[SJF(SRTF)],
    [LRU 及其实现变种],[RR],
    [LFU,MFU],[Priority]
  )
]

== 琐碎概念
- Page-Buffering Algorithms
  - 但一般来说我们不会等到 frame 要去替换了才去行动，而是维持一个空闲帧的池子，当需要的时候直接从池子里取一个即可
    - 感觉像是之前 free frame list 概念的扩充
  - 如果可能的话还可以：
    - 保持 modified pages list
      - 磁盘 idle 时，把其中页写回到磁盘，从而重新加回 free list 里
    - 保持这种 free frames 的完整性，并注意其中的内容
      - 某种程度上作为一种缓存，如果之后在其被重用之前又被用到了，就不用再从 disk 读取（有机会找回被置换写回的页）
- Applications and Page Replacement 考虑页替换和具体应用的关系
  - 所有这些算法都需要 OS 猜测未来的 page access 情况，对一些特定的应用可能有更好的 knowledge 能使用 (i.e. database)
  - *double buffering*: 内存密集型任务可能会导致双缓冲问题，User 和 OS 都缓存了同一份内容，导致一个文件占用了两个帧，浪费了 memory 的空间 (OS as I/O buffer, application for its own work)
  - 可以赋予操作系统直接访问磁盘的权限 (*Raw disk mode*)，绕过 buffering, locking, etc.
- Allocation of Frames
  - 每个进程都至少需要一定数量的 frames，至多当然就是系统总帧数
  - 那么我们该如何分配？主要有两种进程间分配方式
    - Equal allocation：字面意思平均分配
    - Proportional allocation：根据进程的大小按比例分配
    - Linux 其实两个都不是，它是 demand paging
  - 当帧不够用的时候，我们需要替换，分两种
    - Global replacement 可以抢其它线程的帧
      - 可能导致进程执行时间变化剧烈（取决于它人），但吞吐量更好因此更常见
      - 其中一种实现是 Reclaiming Pages：如果 free list 里的帧数低于阈值，就根据 OOM score aggressively Kill some processes。这个策略希望保证有充足的自由内存来满足新的需求
      #fig("/public/assets/Courses/OS/2024-11-19-15-34-24.png",width:50%)
    - Local replacement 只能从自己的帧里选择替换
      - 每个进程性能更一致但可能不充分利用内存
- Non-Uniform Memory Access
  - 不同 CPU 跟不同内存的距离不同，因此访问时间也不同
  #fig("/public/assets/Courses/OS/2024-11-19-15-37-44.png",width:40%)

== Thrashing
- 简单解释
  - 如果我们的进程一直在换进换出页，那么 CPU 使用率会降低
  - 因为进程越多，可能发生一个进程的页刚加载进来又被另一个进程换出去，最后大部分进程都在 sleep
  #fig("/public/assets/Courses/OS/2024-11-19-15-40-04.png",width:40%)
- 更本质的解释
  - demand paging 之所以起效就是因为 *locality*
  - 所以当进程过多，*total size of locality $>$ total memory size*，即 locality 没有被 load 到 frames 里，导致频繁换进换出，自然就发生了 thrashing
- 如何解决 thrashing
  - Option I: 使用 local page replacement
  - Option II: 根据进程的需要分配 locality，使用*工作集模型 (working set model)*来描述
- Working Set Model
  #fig("/public/assets/Courses/OS/2024-11-20-16-27-26.png",width:50%)
  - *Working-set window* $Delta$：固定数目的页引用
    - 太小不能包含整个局部；太大包含多个局部；无穷包含整个进程（接触到的所有页）
  - *Working-set* $WS(t_i)$：$t_i$ 时刻前 $Delta$ 内页的引用情况
  - *Working-set size of process* $P_i$ $WSSi$：最近 $Delta$ 时间内总页面引用次数
  - *Total working sets* $D$：$D = sum WSSi$，近似于 total locality
    - $D>"总可用帧数"$ 就发生颠簸，需要暂停或换出进程
  - 利用 WS 模型，OS 为进程分配大于其 WS 的帧数，如果还有空闲帧就可以启动另一个进程；否则若 WS 之和增加超过总可用帧，就暂停 (suspend) 一个进程（之后再重启）。通过这种方式防止 thrashing，优化 CPU 使用率
- Keeping Track of the Working Set
  - 现在问题就变成如何跟踪 WS 模型，使用定期时间中断和每个页的 reference bit(set to $1$ by hardware when accessed) 来近似
  - 假设 $Delta=10000$ 时间单位，每 $5000$ 时间中断一次，内存中为每个页留 $2bits$ 位置
  - 当中断出现，先复制到内存再清除所有页的引用位
  - 出现 page fault 后，可以检查页引用位和位于内存内的两个位，确定过去 $15000$ 时间内该页是否被引用过
    - 若使用过，至少有一个位为 $1$ 否则全为 $0$
    - 只要有一个 $1$，则可认为处于 WS 中
  - 这种安排*并不完全准确*，因为并不知道在 $5000$ 时间单位的何处出现了引用
    - 通过增加历史位的位数和中断频率可以降低不确定性，但是开销也会变大
- Page-Fault Frequency
  - 每当进程到了一个新的 locality，page fault rate 就会变高
    #fig("/public/assets/Courses/OS/2024-11-20-16-22-35.png", width:30%)
  - 上述方法开销还是略大，很自然地我们会想，用 page fault rate(frequency) 来间接反映 working set。而且对于减少 thrashing 而言这更直接
    - 可以为所期望的页错误设置一个上限和下限，如果太低，说明给资源太多；反之给的资源太少
  - 怎么实现？跟 LRU 很像

== Other Considerations
- Prepaging
  - page fault 时，除了被 fault 的 page，其他相邻的页也一起加载
  - 假设 $s$ 页被 prepaging，其中 $a$ 部分被用到。问题在于节省的 $s*a$ 个页错误的成本是大于还是小于其他 $s*(1-a)$ 不必要的 prepaging 开销
  - 如果 $a$ 接近于 $0$，prepaging 失败，$a$ 接近 $1$，调页成功
- Kernel Memory Allocation
  - 与对待用户内存不同；内核内存从空闲内存池中获取
  - 两个原因：1. 内核需要为不同大小的数据结构分配内存；2. 一些内核内存需要连续
- Page Size
  + 碎片(fragmentation) $arrow.b$
  + 页表大小 $arrow.t$
  + Resolution $arrow.b$
  + I/O 开销 $arrow.t$
  + 页错误数量 $arrow.t$
  + Locality $arrow.b$
  + TLB size and effectiveness $arrow.t$
  + On average, growing over time
- TLB Reach
  - TLB 总共可以索引到的大小，不考虑 TLB 也分级，可以简单计算为
    $ "TLB reach" = ("TLB size") times ("page size") $
  - 理想情况下，每个进程的 WS 应该位于 TLB 中，否则就会导致大量 TLB misses
  - 可以增大页尺寸来缓解 TLB 压力；但可能导致不需要大页尺寸的进程带来的内部碎片
    - 提供多种页大小的支持，但也可能导致 TLB 无法硬件化，性能降低
- Program Structure
  - 程序结构也会影响 page fault，最经典的就是二维数组行主序和列主序访问的例子
- I/O interlock: 把页面锁住，就不会被换出去（比如正在从设备复制文件的那些页）

== Memory management in Linux
- page fault 是针对 user space 的，kernel 分配的内存不会发生 page fault（否则会嵌套）
- 一个进程有自己的 `mm_struct`，所有线程共享同一个页表，内核空间有自己的页表 `swapper_pg_dir`。 这里的 `pgd` 存的是虚拟地址，但当加载到 `satp` 里时会转为物理地址
- Linux *Buddy System*
  - 从物理连续的段上分配内存
  - 每次分配内存都调整或切分到大小是 2 的幂次方，例如请求是 $11KB$，则分配 $16KB$
  - 当释放时，会*合并(coalesce)*相邻的块形成更大的块供之后使用
  - 优劣
    - advantage: 可以迅速组装成大内存（释放后即可合并）
    - disadvantage: internal fragmentation
- *Slab Allocation*
  - Buddy System 管理整页的大内存，但我们需要更细粒度（小于一个 page）的分配。另一方面，随着进程越来越大，即使是 `task_struct` 也变得很大，我们需要有高效管理它们的办法（从一个数据结构快速创建多个 objects）
  - Slab allocator is a *cache of objects*
    - 把多个连续的页面放到一起，将同一大小的 objects 统一分配到这些页面上
    - 比如 `A` 有 $3KB$，但 page 是 $4KB$，最好的办法就是找最小公倍数，用 $3$ 个 page 放 $4$ 个 `A`
    - 进一步，不想每次一个一个初始化，而是一次加载好多个，作为一个 pool 使用，同时也能充当 cache
  - Slab 首先从 partial slab 中分配，若没有则从 empty slab 分配，还没有就 create new empty
  - 优点：没有碎片引起的内存浪费；内存请求可以快速满足

#note(caption: "Takeaway")[
  - Page fault
    - Valid virtual address, invalid physical address
  - Page replacement
    - FIFO, Optimal, LRU, 2nd chance
  - Thrashing and working set
  - Buddy system and slab
]

== Virtual Memory in Linux
- 前面我们讲了那么多，大部分是说 user space 下的实现机制与优化。现在我们回到最开始的 topic，从整体的角度来看，virtual memory 长什么样？它跟 physical memory 的关系如何？
- 对 64bit 架构，完整的虚拟地址空间（实际上没有空间，只是个范围）如下
  #fig("/public/assets/Courses/OS/2024-11-26-14-45-22.png",width:60%)
  - 每个进程有自己的这样一个虚拟地址空间，由各自独立的 Page Table 实现
- 回忆之前的内容，我们现在知道一个程序是以 page 为单位加载的，那为什么程序还有 segment 的概念？
  - 这里的 segment 对应于 elf 的分段，但每个 segment 也是以 page 为单位加载的
  - 或者说，page 是比 segment 更细粒度的分级
  #fig("/public/assets/Courses/OS/2024-11-27-16-28-00.png",width:80%)
- *$32 bits$ and $64 bits$ 地址空间*
  - 对于 $32 bits$ 架构，一共能 handle $4GB$ 的内存，默认情况下 Kernel 使用顶部的 $1GB$，User 使用底部的 $3GB$ (`0 ~ 0xC0000000`)
  - 对于 $64 bits$ 架构，它太大了一般用不完，常用的有 $39, 48, 57$ 版地址空间
    - 就像上图显示的那样，kernel 使用顶部的 $2^64-2^48$ 的内存，从地址开头的 `0xFFFF` 就可以看出其 kernel 身份
    - 此时 kernel 和 user 之间有 gap，如上图所示，以 $39 bits$ 为例
      - `0x0000000000000000 ~ 0x0000007fffffffff` (512GB) :user
      - [architectural gap]
      - `0xffffff8000000000 ~ 0xffffffbbfffeffff` (\~240MB): vmalloc
      - `0xffffffbbffff0000 ~ 0xffffffbcffffffff` (64KB): [guard]
      - `0xffffffbc00000000 ~ 0xffffffbdffffffff` (8GB): vmemmap
      - `0xffffffbe00000000 ~ 0xffffffbffbffffff` (\~8GB): [guard]
      - `0xffffffbffc000000 ~ 0xffffffbfffffffff` (64MB): modules
      - `0xffffffc000000000 ~ 0xffffffffffffffff` (256GB): mapped RAM
- *Low Memory*
  - Kernel 想要 manage RAM，就需要把它整个映射到自己的地址空间里，对 $32 bits$ 架构，kernel 只有 $1GB$
  - 而当年存在一段内核架构发展没跟上物理内存大小发展的历史（物理内存超过 $1GB$ 甚至达到 $8GB$）
  - 如何更改 kernel 的内存分配使得其能够利用更多的内存？一个方法是 PAE (physical address extension, a little ugly)，这里介绍另外一种方法，即 *large memory systems*
    - 如果 physical RAM 小于 $896MB$，就把它 linear map 到 kernel 的低地址空间
      - 这部分称为 kernel Logical Address，对应的 physical RAM 称为 low memory
      - 这里的 VA, PA 转换直接加减一个 `VA_PA_OFFSET` 即可，比 walk page table 快得多
    - 如果 physical RAM 大于 $1GB$，kernel 预留出的 $128MB$ 空间用于 non-contiguous memory mappings，handle 剩余整个 RAM
      - 一个关键点是，kernel 只是 *handle(manage)* 了 physical RAM，而不是 *use*。这 $128MB$ 内存映射到 RAM 后，如果发现 user 要使用内存，就赶紧把这部分 map 到 user，自己就可以 unmap 然后去管理另外的	内存
      - 这块可以复用的部分称为 kernel Virtual Address（也叫 `vmalloc()` area）
    #fig("/public/assets/Courses/OS/2024-11-28-20-34-26.png",width:60%)
  - 但是对于 $64 bits$ 架构，就没有上述问题，比如 $39 bits$ 可以 handle 足足 $512GB$ 的内存
    - 对 $64 bits$ 架构而言，`vmalloc()` area 区域还是用页表映射管理的，为什么不直接用 linear map 控制整个 RAM 呢？
      - [ ] ？
- *Multi-Process*
  - 我们知道 process 有自己各自的页表，对应 User Space 的虚拟地址到物理地址映射，那负责 handle 整个 RAM 的 kernel 怎么办呢？
  - 对 arm 而言比较好理解，如上图，它每个进程有两个 TTBR(PTBR in arm)，切换的是 TTBR0，而 kernel 一直用 TTBR1 不变
  - 但 X86 和 RISCV 就比较难以理解，它们每个进程只有一个 PTBR 指向 PGD
    - 它们的做法是：PGD 变了，但后面指向的内容如 PMD 保持一样（底层的页表共享，但最高层不是）
    - 这就要求我们在进程切换的时候考虑同步问题，比如 $P_0$ 对内核 PGD 映射的内容进行了操作，要同步到其它进程，非常 ugly
    - 我们的实验没有这方面的要求，新进程创建的时候直接复制 `swapper_pg_dir`，后续进程没有 access 到这个级别的地址
  - 注意我们这里说的关系跟之前 thread 那里的一对多、一对一不是一回事 (process level and thread level)
- 三种地址
  - *User Address* 在虚拟上连续，物理上不连续，它需要 walk page table 进行转换
  - *Kernel Virtual Address* 在虚拟上连续，物理上不连续，它需要 walk page table 进行转换
  - *kernel Logical Address* 在虚拟和物理上都连续，可以直接减去 `VA_PA_OFFSET` 得到物理地址，也可以 walk page table
  - 后二者实际上很多时候并不区分，只是在做 coding 的时候我们需要保持清醒（到底能否直接减 `VA_PA_OFFSET`）
  #fig("/public/assets/Courses/OS/2024-11-27-16-17-27.png",width:60%)

