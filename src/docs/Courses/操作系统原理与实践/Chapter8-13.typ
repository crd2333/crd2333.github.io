#import "/src/components/TypstTemplate/lib.typ": *

#show: project.with(
  title: "操作系统原理与实践",
  lang: "zh",
)

#info()[
  - 感觉 #link("https://note.hobbitqia.cc/OS/")[hobbitqia 的笔记] 比较好
  - 还有 #link("https://note.isshikih.top/cour_note/D3QD_OperatingSystem/")[修佬的笔记]，虽然老师不一样
]

#counter(heading).update(7)

= Main Memory
- 从这里开始跨入 OS 内存管理的新纪元
- 背景
  - 程序必须被（从磁盘）拿到内存并放置在进程中才能运行
  - *Main memory* and *registers* are only storage that CPU can access directly，其中 rigister access 只用 one CPU clock (or less)，而 main memory 需要 stall
  - Protection of memory is required to ensure correct operation

== Partition evolution
- In the beginning
  - 最开始我们把一个程序加载到物理内存里，只能执行一个 job，如果 job 比物理内存还大就要分治 divide and conquer，需要 partition
  - 后来我们有多个进程要同时进行，也是使用 partition 的方法，不同的分区执行不同的进程
- Partition requirements
  - *Protection* – keep processes from smashing each other
  - *Fast execution* – memory accesses can’t be slowed by protection mechanisms
  - *Fast context switch* – can’t take forever to setup mapping of addresses
- Physical Memory
  - 问题
    + 不能挪，否则需要更新大量的指针
    + 内存空隙，运行到后期巨量碎片无法使用
  - 从而提出（初版的）Logical Memory，由我们自己定义的地址形式。具体翻译为物理地址由硬件实现
- Logical Memory v1
  - *offset within a partition*
  - Base and Limit registers
    - *Base* added to all addresses
    - *Limit* checked on all memory references 每次访问时检查是否超过了 Limit，如果是就说明越界了
    - Loaded by OS at each *context switch*
    - 每个进程有自己的 base 和 limit 寄存器，每次进程切换时，OS 都会将 base 和 limit 寄存器的值更新为当前进程的值（线程不需要，因为线程是共享的地址空间）
    - Partition 怎么做权限管理？跟后面 segment 类似，有个 table 填 `rwx`
    #grid2(
      fig("/public/assets/Courses/OS/2024-11-05-14-38-15.png", width: 70%),
      fig("/public/assets/Courses/OS/2024-11-05-14-42-54.png")
    )
  - Advantages
    + Built-in protection provided by Limit: No physical protection per page or block
    + Fast execution: Addition and limit check 的实现可以挪到硬件上
    + Fast context switch: Need only change base and limit registers
    + No relocation of program addresses at load time: All addresses relative to zero
    + Partition can be suspended and moved at any time
      - Process is unaware of change. 修改 base 即可移动进程，进程是意识不到的。
      - Expensive for large processes. 移动进程需要改 base，还要把旧的内容全部改到新的位置，耗时
  - 接下来我们思考应该问题，partition 应该多大？

== Memory Allocation Strategies
- Fixed partitions
  - 所有 partition 的 size 是固定的，易于实现
  - 但是要切多大？
    - 如果切的太小，可能有大进程无法加载进来（只能 divide and conquer）
    - 如果切的太大，会有*内部碎片*
- Variable partitions
  - 长度不一致，按需划分。即要给一个进程分配空间时，我们找到比他大的最小的 partition，然后把他放进去
  - 有 3 种分配方法
    + first-fit: allocate from the first block that is big enough
    + best-fit: allocate from the smallest block that is big enough
    + worst-fit: allocate from the largest hole
    - *考一个选择题*
  - Problem – *外部碎片 External Fragmentation*: Variable partitions 可以避免内部碎片，但无论如何总是有外部碎片，在 partition 之外的空闲空间太小，无法被任何进程使用，此时我们需要碎片整理
  - 亦或者，我们可以用 Segmentation 机制来把进程分到多个 section

== Segmentation
- 从这里开始，section, or partition, or segmentation 都是一个概念
- 一个程序分成 text、data、stack 等多个区域，每个区域就用一个 partition 来代表它
- Logical Address v2
  - *`<segment-number, offset>`* segment-number 表示属于第几组，offset 表示 segment 内的偏移量 #h(1fr)
  #fig("/public/assets/Courses/OS/2024-11-05-15-13-12.png", width: 70%)
  - 每个进程有自己的 *Segment register table（段表）*，通过 limit 实现 variable partitions #h(1fr)
    #tbl(columns:3,[base],[limit],[权限],[base],[limit],[权限],[...],[...],[...])
    - 可以看到它额外实现了权限控制（但是跟之后的 paging 机制比，这个权限控制很不细粒度）
- 然而 Segmentation 并没有完全解决外部碎片的问题
  - 我们之后将会利用 fixed partitions 的办法来解决 —— Paging

== Address Binding & Memory-Management Unit
- 到这里我们先做个总结
- 在程序的不同阶段，地址有不同的表现方式：
  - source code addresses are usually symbolic. (e.g., variable name)
  - compiler binds symbols to relocatable addresses. (e.g., “14 bytes from beginning of this module”)
  - linker (or loader) binds relocatable addresses to absolute addresses.
- Logical v.s. Physical Address
  - Logical address – generated by the CPU; also referred to as virtual address.
    - CPU 看不到物理地址，只用逻辑地址，需要经过特定的部件转化为物理地址。
  - Physical address – address seen by the memory unit.
    - 内存单元只能理解物理地址，它是无法改变的。
  - 物理地址对应物理地址空间(Physical Address Space)，逻辑地址对应“逻辑地址空间”(Logical Address Space)，实际上并不存在，而是由一个映射所定义
- MMU
  - 我们之前说要把逻辑地址到物理地址的转换放到硬件上实现，从而不影响速度，这就是 MMU(Memory-Management Unit)
  #grid2(
    fig("/public/assets/Courses/OS/2024-11-05-15-29-00.png"),
    fig("/public/assets/Courses/OS/2024-11-05-15-29-05.png")
  )

#note(caption: [takeaway])[
  - Partition evolution
  - Memory partition-fixed and variable
    - *first, best, worst fit*
    - fragmentation: internal / external
  - Segmentation
    - Logical address vs physical address
  - MMU:address translation protection
]

== Paging
- Fixed 和 Variable 划分都是物理连续的分配，Paging 是把所有内存都变成不连续的，这样空闲的内存不管在哪，都可以分配给进程，避免了外部碎片
- Basic methods
  - Divide *physical* address into fixed-sized blocks called *frames*（物理帧号）
    - Keep track of all free frames.
  - Divide *logical* address into blocks of same size called *pages*（虚拟页号）
  - 页和帧是一样大的，Size is power of 2, usually 4KB
  - 为了跑一个 $N$ pages 的程序，需要找到 $N$ 个 free 的 frames 把程序加载上去
  - 把 $N$ 个帧映射到 $N$ 个页，这个存储帧到页的映射的数据结构叫*页表 page table*
- Paging has no *external fragmentation*, but *internal fragmentation*
  - 那为什么我们要从 variable partition 又回到 fixed partition 呢？因为此时内部碎片问题不严重（一个进程被拆成多个 page，只有最后的页才会有碎片），比之前的 partition 要小很多
    - worst case: 1 frame - 1 byte; average internal fragmentation: 1 / 2 frame size
  - page(frame) size
    - 页如果小，碎片少，但是映射更多，页表需要更大的空间
    - 反之页如果大，碎片多但映射更少，页表较小
    - 现在页逐渐变大，因为内存变得 cheap，一点碎片不影响

=== Page Table
- Page table: Stores the logical page to physical frame mapping
- Frame table: 一个 Bitmap，标记哪些 frame 是空闲的
- 页表不存页号（页号用作索引），只存物理帧号
#fig("/public/assets/Courses/OS/2024-11-05-15-41-57.png", width: 60%)
- Logical Address v3 #h(1fr)
  #fig("/public/assets/Courses/OS/2024-11-05-15-44-39.png", width: 40%)
  - 和之前 Segmentation 机制的 Logical Address 很像，区别在于 fixed partition 和 variable partition
  - *page number (p)*
    - used as an index into a page table
    - page table entry contains the corresponding physical frame number
  - *page offset (d)*
    - offset within the page/frame
    - combined with frame number to get the physical address
- MMU 的变化
  - 首先把 p 拿出来，到页表里读出读出物理帧号，随后和 d 拼接起来就得到了物理地址
  #fig("/public/assets/Courses/OS/2024-11-05-15-48-45.png", width: 60%)

=== Paging Hardware
- 现在我们思考怎么实现页表
- Simple Idea
  - 早期想法：a set of dedicated registers
  - 优势是非常快，但是缺点是寄存器数量有限，无法存储多的页表（如 $32$ bit 地址，$20$ 位作为物理页号，需要 $2^20$ 个页）
- Alternative Way
  - 存储在 main memory 里
  - *page-table base register (PTBR)* 指向页表的起始地址
    - RISC-V 上叫 SATP
    - ARM 上叫 TTBR
    - x86 上叫 CR3
  - *page-table length register (PTLR)* indicates the size of the page table
- 这样每次数据/指令访问需要两次内存访问，第一次把页表读出来，第二次再根据页表去读数据，显然变慢了，如何解决？遇事不决加 cache！如果 hit 了就只用一次内存访问
- *TLB (translation look-aside buffer)* caches the address translation
  - TLB hit: if page number is in the TLB, no need to access the page table.
  - TLB miss: if page number is not in the TLB, need to replace one TLB entry
    - 在 MIPS 上 TLB miss 是由 OS 处理的，但是在 RISC-V 上是由硬件处理的
  - TLB usually use a fast-lookup hardware cache called associative memory
    - Associative memory: memory that supports parallel search
    - Associative memory is not addressed by “addresses”, but contents
      - If `page#` is in associative memory’s key, return `frame#` (value) directly
  - 与页表不同的是，TLB 里存储的既有 page number 又有 frame number，通过比较 page number 来找到对应的 frame number（相当于全相联的 cache）
  - TLB is usually small, 64 to 1024 entries. TLB 数量有限，为了覆盖更大的区域，我们也想要把页变得更大。
- 每个进程有自己的页表，所以我们 context switch 时也要切换页表，要把 TLB 清空(TLB must be consistent with page table)
  - Option I: Flush TLB at every context switch, or,
  - Option II: Tag TLB entries with address-space identifier (ASID) that uniquely identifies a process. 通用的全局 entries 不刷掉，把进程独有的 entries 刷掉
- More on TLB Entries
  - Instruction micro TLB
  - Data micro TLB
  - Main TLB
    - A 4-way, set-associative, 1024 entry cache which stores VA to PA mappings for 4KB, 16KB, and 64KB page sizes.
    - A 2-way, set-associative, 128 entry cache which stores VA to PA mappings for 1MB, 2MB, 16MB, 32MB, 512MB, and 1GB block sizes.
- More on TLB Match Process
  - 不要求掌握
- 现在 MMU 的变化
  - 多了个 TLB
  #fig("/public/assets/Courses/OS/2024-11-06-16-36-25.png",width: 60%)
- Effective Access Time（要会算）
  #fig("/public/assets/Courses/OS/2024-11-06-16-39-17.png",width: 70%)
- 题外话：Segmatation 和 Paging 两个机制其实是差不多时间(1961 and 1962)发明的，后者更优越但硬件上实现更难，所以更晚被广泛使用

=== Page with Memory Protection and Sharing
- Memory Protection
  - 到目前为止，页表里放了物理帧号。我们可以以页为粒度放上保护的一些权限（如可读、写、执行），这样就可以实现内存保护
    - Each page table entry has a present (aka. valid) bit
      - present: the page has a valid physical frame, thus can be accessed
    - Each page table entry contains some protection bits.
      - 任何违反内存保护的行为导致 kernel 陷入 trap
  - XN: protecting code
    - 把内存分为 code 和 data 区，只有 code 区可以执行。e.g. Intel: XD(execute disable), AMD: EVP (enhanced virus protection), ARM: XN (execute never)
  - PXN: Privileged Execute Never
    - A Permission fault is generated if the processor is executing at EL1(kernel) and attempts to execute an instruction fetched from the corresponding memory region when this PXN bit is 1 (usually user space memory). e.g. Intel: SMEP
  #fig("/public/assets/Courses/OS/2024-11-06-16-45-02.png", width: 60%)
- Page Sharing
  - Paging allows to share memory between processes
    - shared memory can be used for inter-process communication
    - shared libraries
  - 同一程序的多个进程可以使用同一份代码，只要这份代码是 reentrant code（or non-self-modifying code:never changes between execution）
  #fig("/public/assets/Courses/OS/2024-11-06-16-56-43.png",width: 50%)

=== Structure of Page Table
- *重要*！！！Page Table 需要物理地址连续(*physically contiguous*)，因为它是由 MMU 去管的，MMU 不知道 logical address 这件事
- 如果只有一级的页表，那么页表所占用的内存将大到不可接受
  - e.g. $32bit$ logical address space and $4KB$ page size. page table 需要 $2^32 \/ 2^12=1M$ 个 entries。如果每个 entry 是 $4bytes$，那一共就是 $4MB$
  - 我们需要有方法压缩页表
    - 考虑到 Logical addresses have holes
    - Break up the logical address space into multiple-level(Hierarchical) of page tables. e.g. two-level page table
    - First-level page table contains the `frame#` for second-level page tables.
  #fig("/public/assets/Courses/OS/2024-11-06-17-12-01.png", width: 60%)
- 最极端的例子($32$ bit, $4$ bytes for each entry)
  #fig("/public/assets/Courses/OS/2024-11-06-17-19-59.png", width: 60%)
  - 页表为什么可以省内存？如果次级页表对应的页都没有被使用，就不需要分配这个页表
    - 关于页表的空间节省计算，可以参考 #link("https://rcore-os.cn/rCore-Tutorial-Book-v3/chapter4/3sv39-implementation-1.html#id6")[rCore-Tutorial-Book]
  - 最坏情况下，如果只访问第一个页和最后一页，那么只用一级页表需要 $1K$ 个页用来放页表（这个页表有 $2^20$ 个条目），但是对于二级页表就只需要 $3$ 个页表（$1$ 个一级和 $2$ 个二级页表），即 $3$ 个页来放页表。内存占用 $4M -> 12K$
- Logical Address v4
  - `<PGD, PTE, offset>`
    - 多级页表每一级的命名规则是，固定最小的是 PTE，最大的是 PGD；如果是更多级页表，PTE 之上是 PMD，再之上是 PUD，再上已经没有名字了所以取了个 P4D
  - a page directory number (1st level page table), a page table number (2nd level page table), and a page offset
  #fig("/public/assets/Courses/OS/2024-11-06-17-22-03.png", width: 60%)
  - 一个比较生草的问题是页表里以及 PTBR 存的是 logical address 还是 physical address，答案肯定是后者，因为我们本来就是在做 LA $->$ PA 的转译，要还是 LA 就“鸡生蛋蛋生鸡”了。同样的理由，page table 内存储的物理地址应该是连续的
  - 另外这里经常出 *page size* 和 *entry size* 变化后的分区大小问题
    - 如果 page size 变大，offset 需要变大，Page Table 能容纳的 entries 也变多；如果 entry size 变大……
- 例如，$64$ bit 下，每个页表 entry size 变为 $8B$，一个页可以放 $2^12\/2^3=512$ entries
  - $64$ bit 能索引的地址太大了，一般都用不完
    - AMD-64 supports $48$ bits; ARM64 supports $39$ bits, $48$ bits
    - 对 $39=9+9+9+12$ bits，有 $3$ 级页表，能索引$1$ GB
    - 对 $48=9+9+9+9+12$ bits，有 $4$ 级页表，能索引$512$ GB
    - 对 $57=9+9+9+9+9+12$ bits，有 $5$ 级页表，已经能索引$128$ PB 了
  #tbl(
    columns: 7,
    [#h(8pt)],[9],[9],[9],[9],[9],[12],
  )

=== Other Page Tables
- 下面我们介绍其它 Page Table
- *Hashed Page Tables*
  - 在地址空间足够大且分布足够稀疏的时候有奇效（因为如果地址空间太大，用 $5$ 级页表最坏情况下要做足足 $5$ 次访存）
  - In hashed page table, virtual `page#` is hashed into a `frame#`
  - 哈希页表的每一个条目除了 page number 和 frame number 以外，还有一个指向有同一哈希值的下一个页表项的指针
  #fig("/public/assets/Courses/OS/2024-11-12-14-40-23.png", width: 60%)
- *Inverted Page Tables*
  - 动机：LA 一般远大于 PA，造成需要 index 的项很多，inverted page table 的想法是去索引 physical frame 而不是 logical pages；另外一个不太重要的原因是，历史上由于 linux 发展较慢，$32 bits$ 只能支持 $4GB$，比 $8GB$ 内存要小
  - 每个 physical frame 对应一个 entry $-->$ 整个 Page Table 占用的内存是固定的！每个 entry 储存 pid 和 page number（对比之前 hierarchical page table 存 frame number，而且它不存 pid 因为每个进程独享自己的 page table），也就是说，Inverted page tables 索引 PA 而不是 LA
  - 现在寻址时，为了 translate LA 到 PA，找到对应有这个 page number 的 entry，不能像原版 page table 那样把 frame number 当做 index 直接找了，必须遍历整个页表找到对应的 pid 和 page number，其在页表中所处的位置即为 frame number
    - 这可以用 TLB 来加速，但是 TLB miss 时代价是很大的
  - 而且这样不能共享内存，因为一个物理帧只能映射到一个页（除非你把每个 entry 做成一个链表进去，也是一种实现）；对比原版 page table，shared memory 只需要两个进程的 page table 指向同一个物理帧号即可
  #fig("/public/assets/Courses/OS/2024-11-12-14-35-28.png", width: 60%)

== Swapping
- 我们前面说 Paging 机制对内存消耗还是比较大的，假如物理内存用完了，能不能把一部分进程放到磁盘上呢？
  - swap out: 用 disk 备份内存，就把 frame 的值交换到 disk 上，然后把 frame 释放出来
  - swap in: 当进程要执行的时候，再把 frame 从 disk 读回来。换回来时不需要相同的物理地址，但是逻辑地址要是一样的
  - 显然这个过程是很慢的，因此当进程在 swap 的时候，会被丢到硬盘的 waiting queue 里
  #fig("/public/assets/Courses/OS/2024-11-12-14-42-26.png", width: 60%)
- Swapping with Paging
  - 为了减轻负担，我们并不是把整个进程塞到 disk，而是部分 page
  - 这样，我们在 load 进程的 disk 部分的同时，进程还在 main memory 的部分可以先执行
    - 换句话说，paging 机制让我们拥有了 partially excuting 一个进程的能力
  #fig("/public/assets/Courses/OS/2024-11-12-14-42-38.png", width: 60%)

== Example: Intel 32- and 64-bit Architectures
- Intel IA-32 支持 Segmentation 和 Paging
- Intel 32 bit 提出 Physical Address Extension(PAE) 来支持 $4GB$ 以上寻址
- 这部分感觉应该不用太详细了解

#hline()
#note(caption: "Takeaway")[
  - Partition evolution
  - Contiguous allocation
    - Fixed, variable
      - first, best, worst fit
      - fragmentation: internal/ external
    - Segmentation
      - Logical address v.s. physical address
  - Fragmentation
    - Internal， external
  - MMU: address translation + protection
  - Paging
    - Page table
      - Hierarchical, hashed page table, inverted
      - Two-level, three-level, four-level
      - For 32 bits and 64 bits architectures
]
#note(caption: "Page table quiz（看看考试是怎么考的）")[
  - In $32 bit$ architecture, $4KB$ page
  + for 1-level page table, how large is the whole page table?
    - $4KB$
  + for 2-level page table, how large is the whole page table?
    + How large for the 1st level PGT?
      - $4KB$
    + How large for the 2nd level PGT?
      - $1K times 4KB = 4MB$
  + Why can 2-level PGT save memory?
    - 允许内存不连续 + 可以按需取用（如果次级页表对应的页没有被使用就不需要分配）
  + 2-level page table walk example
    + Page table base register holds `0x0061,9000`
    + Virtual address is `0xf201,5202` #h(1fr)
      #tbl(columns:3,[PGD$(10)$],[PTE$(10)$],[offset$(12)$],[968],[21],[514])
      - 在 PTBR 中提取 PGD 的地址，然后加上 index 取 PTE 地址……
    + Page table base register holds `0x1051,4000`
    + Virtual address is `0x2190,7010` #h(1fr)
      #tbl(columns:3,[PGD$(10)$],[PTE$(10)$],[offset$(12)$],[134],[263],[16])
      - 在 PTBR 中提取 PGD 的地址，然后加上 index 取 PTE 地址……
      - 题外话：$4KB + 32 bits$ 真的是绝配，对别的 page size、bits 架构就不是这样，如下
  - How about page size is $64KB$
    + What is the virtual address format for 32-bit?  #h(1fr)
      #tbl(columns:3,[PGD],[PTE],[offset],[2],[14],[16])
    + What is the virtual address format for 64-bit?
      - for $39 bit$ VA —— 只能支持两级页表
      #tbl(columns:4,[PGD],[PMD],[PTE],[offset],[],[10],[13],[16])
      - for $48 bit$ VA —— 可以支持三级页表
      #tbl(columns:4,[PGD],[PMD],[PTE],[offset],[6],[13],[13],[16])
  - 以及要学会画 page table walk 的图和过程
]

= Virtual Memory
== Introduction
- 有了 paging 和 page table 的概念，我们能用来做什么？
- Background: 代码需要在内存中执行，但很少需要或同时使用整个程序
  - unused code: error handling code, unusual routines
  - unused data: large data structures
- *partially-loaded*（在 Swapping 部分已经提到这种思想）
  - 我们可以把还没用到的 code 和 data 延迟加载到内存里，用到时再加载
  - 另一个好处是，program size 可以不受 physical memory size 的限制
- 为了实现部分加载，我们有一个虚拟内存（大致上和逻辑地址类似）的概念，主要靠 Paging 来实现
  - 需要注意的是虚拟地址只是范围，并不能真正的存储数据，数据只能存在物理空间里
#grid(
  columns: 2,
  [
    #fig("/public/assets/Courses/OS/2024-11-13-16-25-55.png")
    - 这样，右图的 user stack 就处在连续的虚拟地址下，但它们经页表映射后的帧并不连续，而且不一定都在内存中
  ],
  fig("/public/assets/courses/OS/2024-11-19-14-27-31.png", width:50%)
)

== Demand Paging
- *Demand paging*: 一般 OS 采用的方法是，当页被需要的时候(when it is demanded)才被移进来(page in)，demand 的意思是 access(read/write)
  - if page is invalid (error) $-->$ abort the operation
  - if page is valid but not in physical memory $-->$ bring it to physical memory
    - 这就叫 *page fault*
  - 优劣：no unnecessary I/O, less memory needed, slower response, more apps. 简而言之，用时间换取空间
- 三个核心问题
  - Demand paging 和 page fault 的关系？
    - 前者是利用后者实现的
  - What causes page fault？
    - User space program accesses an address
  - Which hardware issues page fault and Who handles page fault?
    - MMU & OS 后面详细展开
- Demand paging 需要硬件支持：
  + page table entries with valid / invalid bit
  + backing storage (usually disks)
  + instruction restart
- 另外这里我们可以思考 Segmentation 能不能实现 demand paging 机制？其实是不太行的，因为它的粒度太大了，就算实现了效果也不好

== Page Fault
- 比如，C 语言调用 `malloc` 的时候，采用的就是 lazy allocation 策略
  - VMA 是 Virtual Memory Area，malloc 调用 `brk()` 只是增大了 VMA 的大小（修改 vm_end），但是并没有真正的分配内存
    - VMA 这个数据结构类似于 OS 的“账本”
  - 只有当我们真正访问这个地址的时候，会触发 page fault，然后找一个空闲帧真正分配内存，并做了映射
  - 那有没有直接 allocate 的呢？`kmalloc` 会直接分配虚拟连续、物理连续的内存，`vmalloc` 会直接分配虚拟连续、物理不连续的内存
  #fig("/public/assets/Courses/OS/2024-11-13-16-35-45.png",width:70%)
- *MMU issues page fault*，走到页表最低层的时候发现对应的条目的 valid bit 不为 $1$，说明并没有映射，就触发了 page fault
  - $v$ (valid) $->$ frame mapped, $i$ (invalid) $->$ frame not mapped
- *OS handles page fault* (Linux implementation)
  - Page Fault 出现有两种情况（检测是真的 fault 还是只是空头支票没兑现）
    + 一种是地址本身超过了 VMA 的范围，或者落在 Heap 内但权限不对，这种情况操作系统会杀死进程；
      - 为了判断地址是否落在 VMA 里，Linux 使用了红黑树来加速查找
    + 否则，这个时候 OS 就会分配一个 free frame，然后把这个页映射到这个帧上。但这个时候也分两种情况：
      + *Major*: 这个 page 属于 file-backed(e.g. Data, Text)，它不在内存里面，这时需要先从磁盘读取这个 page，然后映射
      + *Minor*: 这个 page 属于 anonymous(e.g. BSS, Heap, Stack)，它本身就在内存里，这时只需要直接映射即可
  #fig("/public/assets/Courses/OS/2024-11-13-16-44-38.png",width:80%)
  - 具体来说就是
    + MMU 先去 access 这个地址，发现 valid bit 是 $i$，issue page fault
    + OS handle page fault，检查之后发现是合法的，分两种情况
      + Major page fault: 把这个页从磁盘读到内存，然后 reset 页表对应的 valid bit $i --> v$
      + Minor page fault: 找一个 free frame 映射到它，省了 $3,4$ 两步，然后 reset 页表对应的 valid bit $i --> v$
    + 这样之后，重新执行一遍指令，MMU 再重新走一遍这个过程，去 access 这个地址，去 TLB 里找就 miss 了（又一个 fault），这时候把它从页表搬到 TLB 里
    #fig("/public/assets/Courses/OS/2024-11-13-16-54-40.png",width:60%)
    - 这个过程里图中 $4$ 最耗时间，因为要读磁盘。如果跟 schedling 结合，此时会把该进程 sleep，丢到 disk 的 waiting queue 里。等 disk 做完了，触发一个 interrupt，然后 OS 会把这个进程移到 ready queue 里
- How to Get Free Frame
  - OS 为内存维护一个 free-frame list
  - Page fault 发生时，OS 从 free list 里拿一个空闲帧进行分配
  - 为了防止信息泄露，在分配时把帧的所有位都置 $0$ (zero-fill-on-demand)
  - 没有空闲的帧怎么办？之后讲 (page replacement)
- Page Fault with swapper
  - 还是说 page replacement 的 case，要把页换进来(swap in)和换出去(swap out)
  - Lazy swapper: 懒惰执行 swap in，只有需要的时候才真正 swap in
    - the swapper that deals with pages is also called a pager.
  - Pre-Paging: pre-page all or some of pages a process will need, before they are referenced.
    - 空间换时间，减少 page fault 的次数（主要是想少掉 major 的），但是如果 pre page 来的没被用就浪费了

#note(caption: [Stages in Demand Paging – Worse Case])[
  + Trap to the operating system.
  + Save the user registers and process state. (pt_regs)
  + Determine that the interrupt was a page fault.
    - Check that the page reference was legal and determine the location of the page on the disk.
  + Find a free frame
  + Determine the location of the page on the disk, issue a read from the disk to the free frame
    + Wait in a queue for this device until the read request is serviced.
    + Wait for the device seek and/or latency time.
    + Begin the transfer of the page to a free frame.
  + While waiting, allocate the CPU to other process.
  + Receive an interrupt from the disk I/O subsystem. (I/O completed)
    + Determine that the interrupt was from the disk.
    + Mark page fault process ready.
  + Handle page fault: wait for the CPU to be allocated to this process again.
    + Save registers and process state for other process.
    + Context switch to page fault process.
  + Correct the page table and other tables to show page is now in memory.
  + Return to user: restore the user registers, process state, and new page table, and then resume the interrupted instruction.
  - 其实跟之前总结得差不太多，只是再结合 context switch
  #fig("/public/assets/Courses/OS/2024-11-13-17-25-11.png",width:70%)
]

== Demand Paging Optimizations
- 先来分析一下 demand paging 的 overhead
  - page fault rate: $0 =< p =< 1$
  - Effective Access Time(EAT):
    $ (1-p) times "memory access" + p times ("page fault overhead" + "swap page out" + "swap page in" + "instruction restart overhead") $
  #fig("/public/assets/Courses/OS/2024-11-13-17-36-32.png",width:60%)
  - 真实场景下，确实可以让减速比 $=< 10%$，因为有 program locality，而且也不是每个 page fault 都是 major
- Discard
  - 仍旧从 disk 读取(page in)，但是对于部分只是拿来读的数据（比如 Code），我们不需要把它写回 disk（写了也是白写），而是直接丢弃，下次直接从 disk 读取（少一次 I/O）
  - 但下列情况还是需要写回
    - Pages not associated with a file (like stack and heap) – anonymous memory
    - Pages modified in memory but not yet written back to the file system
- Copy-on-Write (COW)
  - 我们之前讲 `fork()` 的时候说过，child 从 parent 完全复制，这是很耗时的
  - 我们可以让 child 跟 parent 使用 shared pages，只有当父进程或子进程修改了页的内容时，才会真正为修改的页分配内存（copy 并修改）
  - `vfork` syscall optimizes the case that child calls `exec` immediately after `fork`
  #fig("/public/assets/Courses/OS/2024-11-13-17-45-30.png", width: 60%)

== Page Replacement
- 没有空闲的物理帧时应该怎么办呢？
  - 我们可以交换出去一整个进程从而释放它的所有帧；
  - 更常见地，我们找到一个当前不在使用的帧，并释放它
  - （听起来像是 frame replacement？但其实 frame 一直在那里，只是 page 变了
- Page replacement: find some page in memory but not really in use, and page it out
  - 与物理地址无关 #h(1fr)
  #fig("/public/assets/Courses/OS/2024-11-13-17-48-23.png", width: 60%)

=== Page Replacement Mechanism
- Page Fault Handler (with Page Replacement) 为了 page in 一个 page，需要
  + find the location of the desired page on disk
  + find a free frame:
    + if there is a free frame, use it
    + if there is none, use a page replacement policy to pick a victim frame, write victim frame to disk if dirty
  + bring the desired page into the free frame; update the page tables
  + restart the instruction that caused the trap.
  - 一次 page fault 可能发生 2 次 page I/O，一次 out（可能要把脏页写回）一次 in
  #fig("/public/assets/courses/os/2024-11-19-14-44-37.png",width:50%)

=== Page Replacement Algorithms
- 就像 Scheduling 一样，这里我们也需要对 page 研究算法好坏
  - 之后我们也可以思考一下这跟 scheduling 有什么异同
  - 如何评价？用一串 memory reference string，每个数字都是一个页号，给出物理页的数量，看有多少个 page faults（考试必考）
- 比如
  - FIFO, optimal, LRU, LFU, MFU
  - 下面我们考虑 $7,0,1,2,0,3,0,4,2,3,0,3,0,3,2,1,2,0,1,7,0,1$ 这一串数字
- *First-In-First-Out Algorithm (FIFO)*
  - 替换第一个加载进来的 page
  - $15$ page faults with $3$ frames
  - Belady's Anomaly: For FIFO, 增多 frames 不一定减少 page faults
- *Optimal Algorithm*
  - 如果知道后续页号，替换未来最长时间里不会被用的 Page
  - $9$ page faults with $3$ frames
  - 最优算法，但无法预测未来什么时候会访问这些页，用来评价其它算法的好坏
- *Least Recently Used Algorithm (LRU)*
  - 属于 Time-based 方法，替换最近最少被用的
  - $12$ page faults with $3$ frames
  - 如何实现？基本上有两种方法
    - counter-based，存时间戳，在每次访问时查找最小的页并更新时间戳
    - stack-based，每次访问一个页的时候把它移到栈顶
    - 但这两种方法其实开销都很大，我们有近似的办法，在 PTE 中加了一个 *reference bit*
      - 一开始都设置成 $0$
      - 硬件实现：如果一个 page 被访问，就设置成 $1$
      - 替换时选择 reference bit = 0 (if one exists)
      - 当所有位都设为 $1$ 的时候就只能随机选一个，而且我们无法知道他们的访问顺序
  - LRU 的改进
    - *Additional-Reference-Bits Algorithm*
      - 直觉上，只要我们多设几个 bits，就可以追踪它们的访问顺序。设置 $8$ 个 Bits
      -在一个 time interval (100ms) 之内，对每个 page，refernce bits 每个时刻都右移一位，低位抛弃，高位如果被使用就设成 $1$，否则为 $0$
      - 所以只要比较大小就可以确定该替换哪个
    - *Second-chance algorithm*
      - 给第二次机会，对一个将要被 replaced page，如果它的
        - Reference bit = $0$，那么替换它
        - Reference bit = $1$，把它设置成 $0$，但留在 memory 内，下一次又选到它了才真正换掉它
    - *Enhanced Second-Chance Algorithm*
      - 用 *reference bit* and *modify bit* (if available) 更进一步表征 page 的状态
      - Take ordered pair (reference, modify):
        - $(0, 0)$ neither recently used not modified – best page to replace.
        - $(0, 1)$ not recently used but modified – not quite as good, must write out before replacement
        - $(1, 0)$ recently used but clean – probably will be used again soon
        - $(1, 1)$ recently used and modified – probably will be used again soon and need to write out before replacement
      - 一般是找 $(0,0)$ 的来替换
- *Counting-based Page Replacement*
  - *Least Frequently Used (LFU)* replaces page with the smallest counter
  - *Most Frequently Used (MFU)* replaces page with the largest counter
  - 一般是 LFU 好一些

#note(caption: "与 scheduling 的异同")[
  #tbl(
    columns: 2,
    [Page Replacement],[Scheduling],
    [FIFO],[FCFS],
    [optimal],[SJF(SRTF)],
    [LRU 及其实现变种],[RR],
    [LFU,MFU],[Priority]
  )
]

== 琐碎概念
- Page-Buffering Algorithms
  - 但一般来说我们不会等到 frame 要去替换了才去行动，而是
  - 维持一个空闲帧的池子，当需要的时候直接从池子里取一个即可。系统不繁忙的时候，预先把一些 victime frame 释放掉（写回到磁盘，这样帧可以加到 free list 里）
  - Possibly
    - keep list of modified pages
    - keep free frame contents intact and note what is in them - a kind of cache
  - *double buffering*: 内存密集型任务可能会导致这个问题，User 和 OS 都缓存了同一份内容，导致一个文件占用了两个帧，浪费了 memory 的空间
- Allocation of Frames
  - 每个进程都至少需要一定数量的 frames，那么我们该如何分配？
    - Equal allocation
    - Proportional allocation - Allocate according to the size of process
    - Linux 其实两个都不是，它是 demand paging
  - 当帧不够用的时候，我们需要替换，分两种
    - Global replacement 可以抢其它线程的帧
      - 其中一种实现是 Reclaiming Pages：如果 free list 里的帧数低于阈值，就根据 OOM score aggressively Kill some processes。这个策略希望保证这里有充足的自由内存来满足新的需求
      #fig("/public/assets/courses/os/2024-11-19-15-34-24.png",width:50%)
    - Local replacement 只能从自己的帧里选择替换
- Non-Uniform Memory Access
  - 不同 CPU 距离不同的内存的距离不同，因此访问时间也不同
  #fig("/public/assets/courses/os/2024-11-19-15-37-44.png",width:40%)

== Thrashing
- 简单解释
  - 如果我们的进程一直在换进换出页，那么 CPU 使用率会降低
  - 因为进程越多，可能发生一个进程的页刚加载进来又被另一个进程换出去，最后大部分进程都在 sleep
  #fig("/public/assets/courses/os/2024-11-19-15-40-04.png",width:40%)
- 更本质的解释
  - demand paging 之所以起效就是因为 *locality*
  - 所以当进程过多，*total size of locality > total memory size*，即 locality 没有被 load 到 frames 里，导致频繁换进换出，自然就发生了 thrashing
- 如何解决 thrashing
  - Option I: 使用 local page replacement
  - Option II: 根据进程的需要分配 locality，使用*工作集模型 (working set model)*来描述
    #fig("/public/assets/courses/os/2024-11-20-16-27-26.png",width:50%)
    - 每当进程到了一个新的 locality，page fault rate 就会变高
    #fig("/public/assets/courses/os/2024-11-20-16-22-35.png",width:40%)
    - working set model 描述得很好但不好计算，很自然地我们会想，用 page fault rate 来间接反映 working set。如果太低，说明给资源太多；反之给的资源太少
    - 怎么实现？跟 LRU 很像

== Other Considerations
- Prepaging
  - page fault 时，除了被 fault 的 page，其他相邻的页也一起加载
- Page Size
  + Fragmentation $->$ small page size
  + Page table size $->$ large page size
  + Resolution $->$ small page size
  + I/O overhead $->$ large page size
  + Number of page faults $->$ large page size
  + Locality $->$ small page size
  + TLB size and effectiveness $->$ large page size
  + On average, growing over time
- TLB Reach
  - TLB 总共可以索引到的大小，不考虑 TLB 也分级，可以简单计算为 #h(1fr)
    $ "TLB reach" = ("TLB size") times ("page size") $
- Program Structure
  - 程序结构也会影响 page fault，最经典的就是二维数组行主序和列主序访问的例子
- I/O interlock: 把页面锁住，这样就不会被换出去。

== Memory management in Linux
- page fault 是针对 user space 的，kernel 分配的内存不会发生 page fault（否则会嵌套）
- 一个进程有自己的 `mm_struct`，所有线程共享同一个页表，内核空间有自己的页表 `swapper_pg_dir`。 这里的 `pgd` 存的是虚拟地址，但当加载到 `satp` 里时会转为物理地址
- Linux Buddy System
  - 从物理连续的段上分配内存；每次分配内存大小是 2 的幂次方，例如请求是 11KB，则分配 16KB
  - 当分配时，从物理段上切分出对应的大小（每次切分都是平分）；当释放时，会*合并(coalesce)*相邻的块形成更大的块供之后使用
  - 优劣
    - advantage: 可以迅速组装成大内存（释放后即可合并）
    - disadvantage: internal fragmentation 比如请求是 11KB 但分配 16KB
- Slab Allocation
  - Buddy System 管理整页的大内存，但我们需要更细粒度（小于一个 page）的分配。另一方面，随着进程越来越大，即使是 `task_struct` 也变得很大，我们需要有高效管理它们的办法
  - 当要分配很多 `task_struct`，如何迅速分配？
    - 我们把多个连续的页面放到一起，将 objects 统一分配到这些页面上
    - 比如 `task_struct` 有 $3KB$，但 page 是 $4KB$，最好的办法就是找最小公倍数，用 $3$ 个 page 放 $4$ 个 `task_struct`
  - 进一步，我们不想每次一个 field 一个 field 地 initial，而是一次加载好多个，把它作为一个 pool 来用，另外也能充当 cache 的作用
  - 优点：no fragmentation, fast memory allocation

#note(caption: "Takeaway")[
  - Page fault
    - Valid virtual address, invalid physical address
  - Page replacement
    - FIFO, Optimal, LRU, 2nd chance
  - Thrashing and working set
  - Buddy system and slab
]

== Virtual Memory in Linux
- 前面我们讲了那么多，大部分是说 user space 下的实现机制与优化。现在我们回到最开始的 topic，从整体的角度来看，virtual memory 长什么样？它跟 physical memory 的关系如何？
- 对 64bit 架构，完整的虚拟地址空间（实际上没有空间，只是个范围）如下 #h(1fr)
  #fig("/public/assets/courses/os/2024-11-26-14-45-22.png",width:60%)
  - 每个进程有自己的这样一个虚拟地址空间，由各自独立的 Page Table 实现
- 回忆之前的内容，我们现在知道一个程序是以 page 为单位加载的，那为什么程序还有 segment 的概念？
  - 这里的 segment 对应于 elf 的分段，但每个 segment 也是以 page 为单位加载的
  - 或者说，page 是比 segment 更细粒度的分级
  #fig("/public/assets/courses/os/2024-11-27-16-28-00.png",width:80%)
- *$32 bits$ and $64 bits$ 地址空间*
  - 对于 $32 bits$ 架构，一共能 handle $4GB$ 的内存，默认情况下 Kernel 使用顶部的 $1GB$，User 使用底部的 $3GB$ (`0 ~ 0xC0000000`)
  - 对于 $64 bits$ 架构，它太大了一般用不完，常用的有 $39, 48, 57$ 版地址空间
    - 就像上图显示的那样，kernel 使用顶部的 $2^64-2^48$ 的内存，从地址开头的 `0xFFFF` 就可以看出其 kernel 身份
    - 此时 kernel 和 user 之间有 gap，如上图所示，以 $39 bits$ 为例
      - `0x0000000000000000 ~ 0x0000007fffffffff` (512GB) :user
      - [architectural gap]
      - `0xffffff8000000000 ~ 0xffffffbbfffeffff` (\~240MB): vmalloc
      - `0xffffffbbffff0000 ~ 0xffffffbcffffffff` (64KB): [guard]
      - `0xffffffbc00000000 ~ 0xffffffbdffffffff` (8GB): vmemmap
      - `0xffffffbe00000000 ~ 0xffffffbffbffffff` (\~8GB): [guard]
      - `0xffffffbffc000000 ~ 0xffffffbfffffffff` (64MB): modules
      - `0xffffffc000000000 ~ 0xffffffffffffffff` (256GB): mapped RAM
- *Low Memory*
  - Kernel 想要 manage RAM，就需要把它整个映射到自己的地址空间里，对 $32 bits$ 架构，kernel 只有 $1GB$
  - 而当年存在一段内核架构发展没跟上物理内存大小发展的历史（物理内存超过 $1GB$ 甚至达到 $8GB$）
  - 如何更改 kernel 的内存分配使得其能够利用更多的内存？一个方法是 PAE (physical address extension, a little ugly)，这里介绍另外一种方法，即 *large memory systems*
    - 如果 physical RAM 小于 $896MB$，就把它 linear map 到 kernel 的低地址空间
      - 这部分称为 kernel Logical Address，对应的 physical RAM 称为 low memory
      - 这里的 VA, PA 转换直接加减一个 `VA_PA_OFFSET` 即可，比 walk page table 快得多
    - 如果 physical RAM 大于 $1GB$，kernel 预留出的 $128MB$ 空间用于 non-contiguous memory mappings，handle 剩余整个 RAM
      - 一个关键点是，kernel 只是 *handle(manage)* 了 physical RAM，而不是 *use*。这 $128MB$ 内存映射到 RAM 后，如果发现 user 要使用内存，就赶紧把这部分 map 到 user，自己就可以 unmap 然后去管理另外的	内存
      - 这块可以复用的部分称为 kernel Virtual Address（也叫 `vmalloc()` area）
    #fig("/public/assets/Courses/OS/2024-11-28-20-34-26.png",width:60%)
  - 但是对于 $64 bits$ 架构，就没有上述问题，比如 $39 bits$ 可以 handle 足足 $512GB$ 的内存
    - 对 $64 bits$ 架构而言，`vmalloc()` area 区域还是用页表映射管理的，为什么不直接用 linear map 控制整个 RAM 呢？
      - [ ] ？
- *Multi-Process*
  - 我们知道 process 有自己各自的页表，对应 User Space 的虚拟地址到物理地址映射，那负责 handle 整个 RAM 的 kernel 怎么办呢？
  - 对 arm 而言比较好理解，如上图，它每个进程有两个 TTBR(PTBR in arm)，切换的是 TTBR0，而 kernel 一直用 TTBR1 不变
  - 但 X86 和 RISCV 就比较难以理解，它们每个进程只有一个 PTBR 指向 PGD
    - 它们的做法是：PGD 变了，但后面指向的内容如 PMD 保持一样（底层的页表共享，但最高层不是）
    - 这就要求我们在进程切换的时候考虑同步问题，比如 $P_0$ 对内核 PGD 映射的内容进行了操作，要同步到其它进程，非常 ugly
    - 我们的实验没有这方面的要求，新进程创建的时候直接复制 `swapper_pg_dir`，后续进程没有 access 到这个级别的地址
  - 注意我们这里说的关系跟之前 thread 那里的一对多、一对一不是一回事 (process level and thread level)
- 三种地址
  - *User Address* 在虚拟上连续，物理上不连续，它需要 walk page table 进行转换
  - *Kernel Virtual Address* 在虚拟上连续，物理上不连续，它需要 walk page table 进行转换
  - *kernel Logical Address* 在虚拟和物理上都连续，可以直接减去 `VA_PA_OFFSET` 得到物理地址，也可以 walk page table
  - 后二者实际上很多时候并不区分，只是在做 coding 的时候我们需要保持清醒（到底能否直接减 `VA_PA_OFFSET`）
  #fig("/public/assets/courses/os/2024-11-27-16-17-27.png",width:60%)

= Mass-Storage
- *Magnetic disks* 提供了计算机系统大量的存储空间(secondary storage)，一般来说 hard disk 最主要
- Disk Structure
  - 可见 #link("https://crd2333.github.io/note/Courses/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E7%90%86%E8%AE%BA/")[DB 笔记]
- Performance
  - 总之就是很慢，但读数据本身不算太慢，慢的是 seek 和 rotation，我们要尽量减少

== Disk Scheduling
- 磁盘也需要调度，以减少 access time
  - 以前 OS 会负责调度，现在由 firmware 负责（disk controller）

=== Scheduling Algorithms
- 同样的，我们来衡量几种 scheduling algorithms
  + FCFS
  + SSTF
  + SCAN, C-SCAN
  + LOOK, C-LOOK
  - 我们使用 $\"98, 183, 37, 122, 14, 124, 65, 67\" ([0, 199])$, and initial head position 53 作为例子
  - 注意这里说的是柱面（cyclinder，包含若干等距离的 track），只有不在同一柱面的才需要 seek，同一柱面不同 track 不需要动磁头，不同 sector 就靠转动
- FCFS(First Come First Serve)
  - Total head movements of $640$ cylinders
  - Advantage:
    - Every request gets a fair chance
    - No indefinite postponement
  - Disadvantages:
    - Does not try to optimize seek time
    - May not provide the best possible service
- SSTF(Shortest Seek Time First)
  - 类似 SJF，选择离现在 head position 最近的 request。但是 SSTF 不一定最好(optimal)。可能发生 starvation
  - Total head movements of $236$ cylinders
  - Advantages:
    - Average Response Time decreases
    - Throughput increases
  - Disadvantages
    - Overhead to calculate seek time in advance
    - Starvation may exist
    - High variance : favors only some requests
- SCAN
  - 也叫 elevator 电梯算法（先到一楼把沿途的人都接上，再往高楼走），先扫到一头，再往另一头扫，如果遇到 request 就读取
  - Advantages:
    - Average Response Time
    - High Throughput
    - Low variance of response time
  - Disadvantages:
    - 如果刚好错过电梯，就要等电梯触底再上来，等待时间很长（平均可能少，但最差可能很长）
  - 改进：C-SCAN(Circular-SCAN) is designed to provides a more uniform wait time. 只做单向的扫，到达一端时立刻回到开头，随后从底往上扫，这样最多只用等待一圈
  - 另外明显有一个优化点，即 LOOK
- LOOK
  - 在 SCAN / C-SCAN 的基础上，只走到一端最后一个任务（look 是否有请求）而不走到 disk 的头
  - LOOK is a version of SCAN, C-LOOK is a version of C-SCAN
  - Advantage:
    - Prevents the extra delay which occurred due to unnecessary traversal to the end of the disk
#note(caption: "Selecting Disk-Scheduling Algorithm")[
  - 依赖于请求的模式，而请求本身又依赖于文件分配策略。文件系统如果注重空间局部性，能够提供很好的表现提升
  - 如果 I/O 比较少，FCFS 和 SSTF 即可。如果是大型的服务器或者数据库，一般使用 C-LOOK。如果是 SSD（不用 seek），一般使用 FCFS
]

== Other Storage Devices
- *Nonvolatile Memory Devices*
  - 但其实上面讲了这么多，都有点过时了，因为现在有了固态硬盘(solid-state disks, SSD)
  - 与磁盘相比，寿命短，容量小，速度快（Bus 慢，需要直接连到 PCIE 上）
  - 没有 arm 也不需要转，因此不存在 seek time 和 rotational latency，所以一般用 FCFS
- *Magnetic Tape 磁带*
  - 容量很大很便宜
  - 但是很慢。因为需要倒带，一般都做顺序访问而不是随机访问。现在主要用来做备份

== Disk Management & Attachment
- 使用这些介质（磁盘、固态硬盘、磁带）的时候，需要先格式化
- Physical formatting: 把介质上分好不同的部分
- Boot block initializes system
- swap space management
- Disks can be attached to the computer as:
  - host-attached storage
    - hard disk, RAID arrays, CD, DVD, tape...
    - 通过 I/O bus 直接插到主机上
  - network-attached storage
    - 通过网络连接
  - storage area network
    - 做成一个阵列，通过局域网连接
- 都不是很重要

== RAID
- 动机
  - HDDs 越来越小和便宜，但还是很容易坏
  - 如果一个系统可以拥有大量磁盘，那么就能改善数据的读写速率（因为可以并行）和可靠性（使用冗余来降低出现错误的期望）
  - 这样的磁盘组织技术称为*磁盘冗余阵列(Redundant Arrays of Independent Disk, RAID)*技术
- 几个关键概念
  - Data Mirroring: 其实就是数据有多个备份
  - Data Striping: 把数据分成多个部分，分别存储在不同的磁盘上
  - Error-Code Correcting (ECC) - Parity Bits: 通过奇偶校验位来检测和纠正错误
- RAID 0
  - 根据固定的 striped size 把数据分散在不同的磁盘，没有做任何 redundancy
  - Improves performance, but not reliability
  - e.g. 5 files of various sizes, 4 disks
  #fig("/public/assets/courses/os/2024-12-03-14-34-00.png",width:30%)
- RAID 1
  - 也被称为 mirroring，存在一个主磁盘，一个备份磁盘。主磁盘写入数据后，备份磁盘进行完全拷贝
  - A little improves performance（可以从两个读）, but too expensive
  - e.g. 5 files of various sizes, 4 disks
  #fig("/public/assets/courses/os/2024-12-03-14-35-48.png",width:30%)
- RAID 2
  - stripes data at the bit-level; uses Hamming code(4bit data + 3bit parity)  for error correction
  - 没有被实际应用，因为粒度太小，现在无法单独读出来一个比特，至少读出一个字节
  #fig("/public/assets/courses/os/2024-12-03-14-37-07.png",width:30%)
- RAID 3
  - Bit-interleaved parity: 纠错码单独存在一个盘里
- RAID 4, 5, 6
  - RAID 4: Basically like RAID 3, but interleaving it with strips (blocks)
    - Block-interleaved parity，纠错码单独存在一个盘里，且用块来做 strip
  - RAID 5: Like RAID 4, but parity is spread all over the disks as opposed to having just one parity disk
    - parity bit 被分散地存到了不同的磁盘里。相比于 RAID 4，每个盘的读写比较均衡
  - RAID 6: extends RAID 5 by adding an additional parity block
    - 又加了一个 parity bit，也是分散存储
  #fig("/public/assets/courses/os/2024-12-03-14-42-45.png",width:40%)
- 注意：RAID 只能 detect/recover from disk failures，但无法 prevent/detect data corruption
- ZFS adds checksums to all FS data and metadata.
  - 这样可以检验磁盘是否写错

#note(caption: "Takeaway")[
  - Disk structure
  - *Disk scheduling*
    - FCFS, SSTF, SCAN, C-SCAN, LOOK, C-LOOK
  - RAID 0-6
]

= I/O Systems
- I/O 设备是 OS 的一个 major 组成部分，比下面左图画的大得多
  #grid2(
    fig("/public/assets/courses/os/2024-12-03-14-48-07.png"),
    fig("/public/assets/courses/os/2024-12-03-14-48-12.png")
  )
- Common concepts: signals from I/O devices interface with computer
  - bus: 用来做设备和 CPU 的互连
  - port
  - controller
- direct I/O instructions & memory-mapped I/O
  - CISC 包含 dedicated I/O instructions，如 x86 的 `in` 和 `out`
  - 而 RISC 把 I/O instructions 存在 memory 里，即 memory-mapped I/O

== I/O access
- I/O access can use *polling or interrupts*
  - 本章最大的重点就在这
  - Polling 轮询: CPU 不断地主动询问 I/O 设备是否 ready
    - Polling requires busy wait，而 busy wait 需要锁，会 sleep
    - 如果 device 很快那么轮询是合理的，否则会很低效
  - Interrupts: I/O 设备准备好后发出中断信号，CPU 响应
    - 当线/进程 $T_1$ 需要等待 device 时，被加入到 device 的 waiting queue 上，然后 scheduling 到别的线/进程 $T_2$ 继续工作（跟 polling 相比，这时它没在 sleep 而在做别的工作）
    - 收到 interrupt 后，$T_1$ 被加回到 ready queue 上
    - interrupt 这里也会有一个 interrupt table
    #fig("/public/assets/courses/os/2024-12-03-15-17-13.png",width:50%)
    - 所以如果中断发生的频率很高，那么上下文切换会浪费很多 CPU 时间
- DMA(Direct Memory Access)
  - 为了减少 CPU 的负担，我们可以让 I/O 设备直接访问内存，而不需要 CPU 介入。于是就把权限下放到 device driver, drive controller, DMA controller, memory controller, bus controller
  - 所有 driver 在 CPU 上跑，controller 在设备上跑
  #fig("/public/assets/courses/os/2024-12-03-15-29-14.png",width:70%)

== Application I/O Interface
- I/O system calls encapsulate device behaviors in generic classes
  - in Linux, devices can be accessed as *files*; low-level access with `ioctl`
- Device driver 层对 kernel 隐藏了 I/O  controllers 之间的差异，做了一层抽象（统一借口）
  #fig("/public/assets/courses/os/2024-12-03-15-40-43.png",width:60%)
- Devices vary in many dimensions
  #tbl(
    columns:3,
    [aspect],[variation],[example],
    [data-transfer mode],[block, character],[disk, terminal],
    [access method],[sequential, random],[modem, CD-ROM],
    [transfer schedule],[synchronous, asynchronous],[tape,keyboard],
    [sharing],[dedicated, shared],[tape, keyboard],
    [device speed],[latency, seek time, transfer rate, delay between operations],
    [I/O direction],[read-only, write-only, read-write],[CD-ROM, graphics controller, disk],
  )

== Kernel I/O Subsystem
- 相当于是在 I/O 侧又做了一遍 OS 那套东西
  - I/O scheduling
  - Buffering - store data in memory while transferring between devices.
  - Caching: hold a copy of data for fast access.
  - Spooling: A spool is a buffer that holds the output (device’s input) if device can serve only one request at a time.
  - Device reservation: provides exclusive access to a device.
  - Error handling

== Between Kernel and I/O
- I/O Protection
  - 需要 privilege，OS 去执行 I/O 操作(via syscalls)
  - Kernel Data Structures #h(1fr)
    - Kernel keeps state info for I/O components
    - Some OS uses message passing to implement I/O (e.g. Windows)
    #fig("/public/assets/courses/os/2024-12-04-16-26-37.png",width:40%)
      - 所有东西都被抽象成 file，后面我们会讲
- I/O Requests to Hardware #h(1fr)
  #fig("/public/assets/courses/os/2024-12-04-16-25-53.png",width:60%)
- 总之，这里如果细细展开的话也会很复杂（kernel 和 I/O 的数据结构与交互等），但不是重点

== Performance
- I/O 是计算机系统中的 major 部分，因此也极大影响了 performance
- Improve Performance
  - Reduce number of context switches
  - Reduce data copying
  - Use smarter hardware devices, such as reducing interrupts by using large transfers, smart controllers, polling
  - Use DMA
  - Balance CPU, memory, bus, and I/O performance for highest throughput
  - Move user-mode processes / daemons to kernel threads

#note(caption: "Takeaway")[
  - IO hardware
  - IO access
    - *polling, interrupt*
  - Device types
  - Application I/O Interface
  - Kernel IO subsystem
]
- 注：Mass Storage 和 I/O Systems 两节不算特别重要，把 Takeaway 里的内容看一看就差不多了

= File System Interface
== File concept
- 现在我们有了大规模存储介质和 I/O 的概念，但是如何使用？OS 将这一切*抽象*成*文件系统*
  - Abstract
    - CPU is abstracted to #underline[Process]
    - Memory is abstracted to #underline[Address Space]
    - Storage is abstracted to #underline[File System]
      - file $->$ track / sector
  - 核心问题
    - How to use file system?
      - How to use file?
      - How to use directory?
    - How to implement file system?
      - How to implement file?
      - How to implement directory?
- File is *a contiguous logical space* for storing information
  - e.g. database, audio, video, web pages...
  - 有不同类型的 files
    - data: character, binary, and application-specific
    - program
    - special one: proc file system，也就是 linux `/proc` 目录下的那些数字文件夹，使用 file-system interface 来检索系统信息
- File Attributes
  - Name – only information kept in human-readable form
  - Identifier – unique tag (number) identifies file within file system
  - Type – needed for systems that support different types
  - Location – pointer to file location on device
  - Size – current file size
  - Protection – controls who can do reading, writing, executing
  - Time, date, and user identification – data for protection, security, and usage monitoring
  - ...
  - 这些信息是目录结构(directory structure)的一部分，保存在 `FCB` 数据结构里（联系 process 的元信息存在 `PCB` 里），也存在磁盘上。可能有其他属性，例如 checksum，这些会存到 extended file attributes 里
  - linux 上使用 `stat`(statistic) 系统调用可以获取这些信息
- File Operations
  - *create*:
    - space in the file system should be found
    - an entry must be allocated in the directory
  - *open*: most operations need to file to be opened first
    - return a handler for other operations
    - 一些文件系统提供 lock 机制
      - Two types of locks: shared lock and exclusive lock
      - Two locking machanisms:
        - mandatory lock: 一旦进程获取了独占锁，操作系统就阻止任何其他进程访问对应文件
        - advisory lock: 进程可以自己得知锁的状态然后决定要不要坚持访问
  - *read/write*: need to maintain a pointer
    - reposition within file – seek. 将 current-file-position pointer 的位置重新定位到给定值，例如文件开头或结尾。
  - *close*
  - *delete*
    - Release file space
    - Hardlink: maintain a counter - delete the file until the last link is deleted
  - *truncate*: 把文件的所有 content 清空，但保留 metadata。
- File Types
  - 识别不同的文件类型一般通过：
    - as part of the file names(file extension): 例如规定只有扩展名是 .com, .exe, .sh 的文件才能执行
    - magic number of the file
      - 在文件开始部分放一些 magic number 来表明文件类型
      - 例如 `7f454c46` 是 ASCII 字符，表示 ELF 文件格式
- File Structure
  - 一个文件可以有不同的结构，由 OS 或 program 指定
    - No structure: a stream of bytes or words
    - Simple record structure: Lines of records, fixed length or variable length
    - Complex structures
- Access Methods
  - Sequential access
    - 每次都只能从头开始访问，比如用 tape 实现的文件系统
  - Direct access
    - 可以跳到任意的位置访问，也称为随机访问
    - 在直接访问的方法之上，还有可能提供索引，即先在索引中得知所需访问的内容在哪里，然后去访问。也有可能使用多层索引表

== Directory structure
- 一开始只有文件，但文件想重名怎么办，就想把它们 subdivided into *partitions*
  - partitions also known as *minidisks, slices*
  - 一个文件系统可以有多个 disk，一个 disk 可以有多个 partition，一个 partition 又有自己的文件系统
  - disk or partition can be used raw(without a file system)，即 partition 也可以不对应一个文件系统
- Directory
  - is a collection of nodes containing information about all files
  - 可以把 directory 当成特殊的 file，其内容是 file 的集合
  #fig("/public/assets/courses/os/2024-12-04-17-09-17.png",width:50%)
  - Operations Performed on Directory: Create / delete a file, list a directory, search for a file, traverse the file system
  - target: 要能快速定位文件；要兼顾效率、便于使用、便于按一些属性聚合
- Directories implementation
  #grid(
    columns: 2,
    fig("/public/assets/courses/os/2024-12-04-17-15-36.png",width:80%),
    fig("/public/assets/courses/os/2024-12-04-17-15-43.png",width:80%),
    fig("/public/assets/courses/os/2024-12-04-17-16-05.png",width:80%),
    fig("/public/assets/courses/os/2024-12-04-17-16-12.png",width:80%),
    fig("/public/assets/courses/os/2024-12-04-17-26-41.png",width:80%)
  )
  - A single directory for all users
    - 存在 Naming problems and grouping problems，如果两个用户想用相同的文件名，无法实现
  - Two-Level Directory
    - 为每个用户分出单独的 directory，不同用户可以有同样名字的不同 file
      - Each user has his own user file directory (UFD), it is in the master file directory (MFD)
    - Efficient to search
  - Tree-Structured Directories
    - efficient in searching, can group files, convenient naming
    - 如果所需目录不在当前目录，那么用户就必须提供一个路径名(path name, absolute or relative)来指定
    - 不能 share 一个文件（即多个指针指向同一个文件），因为这样就会形成一个图而不是树
  - Acyclic-Graph Directories
    - allow links to a directory entry/files for aliasing (no longer a tree)
    - Dangling pointer problem
      - e.g., if delete file `/dict/all`, `/dict/w/list` and `/spell/words/list` are dangling pointers 悬垂指针
      - solution: back pointers/reference counter，如果一个文件被删除，那么它的 reference counter 就会减一，当减到 0 时，才真正删除
  - General Graph Directory
    - 更进一步，允许目录中有环
    - garbage collection: 如果没有外界目录指向一个环，那么就把这个环都回收了
    - 每次设置一个 new link 的时候都使用 cycle detection algorithm

== Others
- File System Mounting
  -
- File Sharing
  - shared 文件需要有一定的保护，规定 User IDs, Group IDs 允许某些用户、某些组的用户访问
  - remote file sharing: 在分布式系统里，文件可以通过网络来共享
- Protection
  - 文件的所有者/创建者应该能控制文件可以被谁访问，能被做什么
  - Access Control List (ACL)
    - 给每个文件和目录维护一个 ACL，定义三种 users(owner, group, other) 和三种 access(R,W,X)
    - linux 上使用 `getfacl` 和 `setfacl`，或者 `chmod`, `chgrp` 这些命令
    - 优点是可以提供细粒度的控制
    - 缺点是如何构建这个列表，以及如何将这个列表存在目录里

#note(caption: "Takeaway")[
  - File system
  - File operations
    -  Create, open, read/write, close
  - File type
  - File structure
  - File access
  - Directory structure
    - Single level, two-level, tree, acyclic-graph, general graph
  - Protection
    - ACL
]

= File System Implementation
== Layered File System
#wrap-content(
  align: right,
  column-gutter: 2em,
  fig("/public/assets/courses/os/2024-12-04-17-45-28.png",height:25em)
)[
  - OS 对文件系统做这样的层次化封装，主要是为了通过接口来隔离不同层，降低每一层的 complexity and redundancy，但是增加了 overhead 和 performance
    - Logical file system
      - Keep all the meta-data necessary for the file system
      - It stores the directory structure
      - It stores a data structure that stores the file description (File Control Block - FCB)
      - Input from above: Open/Read/Write filepath
      - Output to below: Read/Write logical blocks
    - File-organization module
      - Knows about logical file blocks (from 0 to N) and corresponding physical file blocks: it performs translation. 把逻辑块映射到物理块。输入是逻辑块号，输出是物理块号
      - It also manages free space
    - Basic file system
      - Allocates/maintains various buffers that contain file-system, directory, and data blocks. 提供 buffer，用于缓存文件系统、目录和数据块。在 Linux 中称为 IO buffer
    - I/O Control
      - 将上层的指令转换为 low-level, hardware-specific 的指令来实现相关操作。同时也可以发中断
  ]

== File System Data Structures
- On-disk structures
  - 可持久化的(persisitant)
  - An optional *boot control block*
  - A *volume control block*
  - A *directory*
  - A *per-file File Control Block (FCB)*
- In-memory structures
  - 易失的(volatile)
  - A *mount table* with one entry per mounted volume
  - A *directory cache* for fast path translation (performance)
  - A *global open-file table*
  - A *per-process open-file table*
  - Various *buffers* holding disk blocks “in transit” (performance)
- File Control Block
  - FCB 之于文件系统就像 PCB 之于进程，是非常重要的
  - 在 UNIX 中，FCB 被称为 inode；在 NTFS 中，每个 FCB 是一个叫 master file table 的结构的一行
  - 左图为典型的 FCB 结构所包含的信息
  - 右图为 `ext2_inode` 的结构，前面的是 metadata，后面存有数据块的指针
  #grid2(
    fig("/public/assets/Courses/OS/2024-12-10-14-22-22.png",width:60%),
    fig("/public/assets/Courses/OS/2024-12-10-14-22-30.png")
  )

== File Creation & Open & Close
- *File Creation*
  - 逻辑文件系统为这个新的文件分配一个新的 FCB（与文件一对一映射），随后把它放到一个目录里，将对应的 directory 读到内存，并用 filename 和 FCB 更新 directory
- *File Open*
  - 系统调用 `open()` 将文件名传给 logical file system，后者搜索 *system-wide(global) open-file table* 以确定该文件是否正在被其他进程使用。
    - 如果有，则直接在当前进程的 per-process open-file table 中新建一个 entry，指向 system-wide open-file table 中的对应项即可，并且 increment the *open count*
    - 否则，需要在 directory 中找到这个 file name，将其 FCB 从磁盘加载到内存中，并将其放在 system-wide open-file table 中。然后，在当前进程的 per-process open-file table 中新建一个 entry，指向 system-wide open-file table 中的对应项
    - （这里的 index 就是 file descriptor
  #fig("/public/assets/Courses/OS/2024-12-10-14-28-23.png",width:70%)
- *File Close*
  - 至于关闭，就是 Open 的逆过程
  - per-process open-file table 中对应 entry 将被删除，system-wide open-file table 中的 counter 将被 $-1$
  - 如果该 counter 清零，则更新的 metadata 将被写会磁盘上的 directory structure 中，system-wide open-file table 的对应 entry 将被删除
- 在 Unix(UFS) 里面 System-Wide Open-File Table 会放设备、网络，所以我们的设备也是用文件来表示的，读写文件相当于读写设备

#note(caption: "我们现在学过几种 Table？")[
  + Segment Table
  + Page Table
    + Hierarchical Page Table
    + Hashed Page Table
    + Inverted Page Table
  + Syscall Table
  + File Table
  - 思考：每个 table 长什么样（包含什么）？它们的 number 代表什么含义？
]

== Virtual File Systems
- 操作系统可以同时支持多种类型的文件系统
  - e.g. FAT32, NTFS, Ext2/3/4
  - but how？
    - 一个叫 David Wheeler 说过: All problems in computer science can be solved by another level of indirection#strike[, except for the problem of too many layers of indirection]
    - 也就是分层抽象，我们加一层 virtual file system (VFS)
  #fig("/public/assets/Courses/OS/2024-12-10-14-34-36.png",width:50%)
- VFS provides an *object-oriented way* of implementing file systems
  - 操作系统为 FS 定义一套 common interface，所有 FSEs 需要实现它们
  - Syscall 基于 common interface 实现
- VFS Implementation
  - e.g.: Write syscall $->$ `vfs_write` $->$ indirect call $->$ `ext4_file_write_iter`
  #fig("/public/assets/Courses/OS/2024-12-10-14-43-43.png",width:70%)
  - 在需要调用某个函数时，去对应 FS 的函数表的约定位置找到函数指针就可以访问了。这和 C++ 中多态的虚函数表是类似的
  - 在创建这个文件的时候，`file->f_op` 就被设为了对应的函数表的地址（`f_op` 是指针）
  - `struct file` 里存了文件的 `file_operations`，但没有存文件的 type，因为我们有知道操作对应 fs 的文件，就不需要知道文件的类型了

== Directory Implementation
- 在 Linux 上，Directory 就是一个 special file，存储 file name 到 inode 的映射
  - L: Directory == File; W: Directory != File
  - Linux 这样的优点是接口相同，不用额外设计一套；缺点是容易搞混。这跟 Process V.S. Thread 类似，windows 把它们分开，而 linux 都用 `task_struct` 描述
- 我们这里讲的是 linux 实现
  - 它的数据块有自己的名字（目录项 `dir_entry`），每一个目录项有一个 inode 号、目录项长度、名字长度
    - 目录项 $4$ 对齐是为了重用，比如删除了 `a` 又创建 `bb`，可以直接重用
    - 之所以要存“目录项长度”，是为了加速搜索，典型的用空间换时间
  #fig("/public/assets/Courses/OS/2024-12-10-14-51-24.png",width:70%)
  - e.g. `/home/stu/a, bb, ccc, test`，对 `/home/stu` 这个目录的数据块：
    - How many entries? $4$ 个
    - What's the structure? 比如对 `a` 这个目录项，inode 号占 $4$ 个 bytes，目录项长度占 $2$ 个 bytes，长度为 $9=4+2+2+1 -> 12$（$4$ 对齐），文件名长度占 $2$ 个 bytes，长度为 $1$，文件名为 `a` 占 $1$ 个 byte
    - How to open `a`? 先去 `/` 对应的数据块里找 `home` 的文件名，拿出这个目录项的 inode；然后去 inode 指向的数据块里继续……
- 遍历
  - 最简单的实现方式是 linear list，即维护 `dir_entry[]`，这种方案的缺点是查找文件很费时
  - 使用有序数据结构（有序表、平衡树、B+ 树等）能够优化这一问题
  - 或者使用 hash table，尤其有利于那种经常访问小碎片文件的情况
- 创建
  - Consider directory and FCB
  - 先分配一个新的 inode，然后找到当前目录的 inode，在其指向的数据块里加上一个目录项

== Disk Block Allocation
- Files need to be allocated with disk blocks to store data，这里介绍 3 种不同的 policy

=== Allocation Policies
#wrap-content(
  align: right,
  [
    #fig("/public/assets/Courses/OS/2024-12-10-15-15-40.png",width:70%)
    #fig("/public/assets/Courses/OS/2024-12-10-15-29-27.png",width:70%)
    #fig("/public/assets/Courses/OS/2024-12-10-15-29-14.png",width:70%)
  ],
  [
    - *Contiguous Allocation*
      - Each file is in a set of contiguous blocks
      - Can be difficult to find free space: Best Fit, First Fit, .etc
      - 优点是顺序访问很快，同时目录也只需要维护文件的起始 block 及其长度；
      - 缺点跟之前类似，会带来 external（但不同的是，这里磁盘通常够大，我们可能不在意这个问题）；另外文件可能会增大，需要重新分配空间（这是数组的问题，那就自然引出 linked list）
    - *Linked Allocation*
      - Linked allocation: each file is a linked list of disk blocks
      - 优点：允许块分布在磁盘上的任何地方，只需要维护每个块的下一个块的地址即可。这样就没有外部碎片了
      - 缺点是定位某个块需要遍历链表，需要很多 IO；而且 reliablity 也不好，如果某个块的指针坏掉，后面的块就都访问不到了；而且这种实现方式不支持 random access
    - *Indexed Allocation*
      - Indexed allocation: each file has its own index blocks of pointers to its data blocks
      - 用一个块只做 index，里面存放指向数据块的指针
      - 优点是这样可以支持 random access
      - 缺点是 reliability 不好，如果 index 块坏了，文件就访问不到了；并且浪费空间（需要一个块做 index）
      - 需要一个方法分配 index block 的大小（太大会浪费，太小那么指向的空间小）。我们可以把 index block 链接起来，用多级索引
    - *Multi-level Indexed Allocation* (combined scheme, like Page Table)
      - 我们之前说的 inode 实际上就是这里的 index block
      - direct block 最快，indirect blocks 需要更高的 seek time（做了个权衡）
      - 需要会算它在不同 Page Size 下能 index 的大小
      #fig("/public/assets/Courses/OS/2024-12-10-15-23-17.png",width:90%)
    ]
  )

=== Free-Space Management
#wrap-content(
  align: right,
  [
    #fig("/public/assets/Courses/OS/2024-12-10-15-48-44.png",width:80%)
  ],
  [
    - *Bitmap*
      - 每一个 block 都用一个比特记录分配状态
      - 容易找到连续的空闲空间，但是占用额外空间
      - 额外空间占用计算
      #fig("/public/assets/Courses/OS/2024-12-10-15-30-36.png",width:50%)
      - Clutster 优化：如果每 $4$ 个 block 用一个 bit 表示，能稍微少一些
    - *Linked Free Space*
      - Keep free blocks in linked list
      - 好处是不会浪费空间
      - 缺点是不能快速找到连续的空闲空间，效率低；以及 reliability 不好，断一个后续都无法访问
  ]
)
- *Group*
  - Use indexes to group free blocks. Store address of n-1 free blocks in the first free block, plus a pointer to the next index block
  - 跟 bitmap 一样，加大粒度。维护若干个 block 形成的链表，每个 block 保存若干空闲块的地址
- *Counting*
  - a link of clusters (starting block + the number of contiguous blocks)
  - 维护的是连续的空闲块的链表，即链表的每个结点是连续的空闲块的首块指针和连续空虚块的数量

== File System Performance and Reliability
- To improve file system performance
  - Keeping data and metadata *close together*
  - Use *cache*
    - *asynchronous writes*，也是写到 cache 里，然后异步写到磁盘
    - *Free-behind and read-ahead*: read-ahead 把后面的 block 也读到 cache 里
    - 一个异常现象 —— Reads slower than write，这是因为 read 通常是对新文件，而 write 可以直接写到 cache 里
    - Page Cache
- Recovery
  - *Log Structured File Systems*

#takeaway[
  - File system layers
  - File system implementation
    - On-disk structure, in-memory structure
    - *inode*
  - File `creation()`, `open()`
  - *VFS*
  - Directory Implementation
  - *Allocation Methods*
    - Contiguous, linked, indexed
  - Free-Space Management
]

== File System Implementation --- In Practice
- Two key Abstraction
  - *File*
    - A linear array of bytes
    - External name (visible to user) must be symbolic
    - Internal name (low-level names): inode number in UNIX
  - *Directory*
    - translation from external name to internal name
    - 每个 entry 要么存储 pointer to file，要么是其它 directory
- File Descriptor
  - 每个进程有自己的 file descriptor table，file descriptor 是其中的索引
  - 每个 file descriptor entry 包含一个 file object（即一个打开的文件），指向一个 inode
  - 简单来说，就是一个用来索引已打开文件的数字
  #fig("/public/assets/Courses/OS/2024-12-11-20-37-18.png",width:40%)
- Getting information about files
  - linux 的 file information 存储在 `struct stat` 中
  - 通过同名 `stat` 命令进行查看
- Link (hard V.S. soft)
  - 当我们在 `remove` or `delete` 一个文件时，如果用 `strace` 会发现它使用的是 `unlinkat` 系统调用
  - 一个文件可能被多个目录以多个名字引用，这就是 link，有 hard 和 soft 两种
  - A *hard link* is a *directory entry* that associates with a file
    - 比如 `.` 是 directory 本身的 hard link，`..` 是 parent directory 的 hard link
    - `ln file link` 创建一个 hard link，实际上就是在 directory 的数据块里面加了一个 inode 一样但 file name 不一样的目录项
      - 这个 `link` 可以跟 `file` 一样被执行
      - 因此当我们 `rm file` 时，只是删除了一个目录项，由于 hard link 的存在，文件并不会真正删除而只是计数减一
  - A *soft link* is a *file* containing *the path name of another file*
    - soft link 也叫 symbolic link or symlink
    - 比如，`ln -s file link` 创建一个 soft link，它的 inode 跟 `file` 不同，它作为一个文件存储了 `file` 的路径
      - 因此当我们 `rm file` 时，soft link 还存在但会失效，对它进行操作会爆 `No such file or directory` 错误
  - soft link 可能指向 directories，但是 hard link 不行；而且 soft link 能够 cross filesystem boundaries（因为不是靠 inode 访问而是靠 path name）；两种 link 比起来，hard link 更经济但没 soft link 灵活
- 实际的 File System Organization 例子
  - 需要存储 data block, inode, bitmap, superblock
  - data block 最大，给它分配 $56$
  - 假设每个 inode 占 $256 bytes$，$4KB$ 可以放 $16$ 个 inode，那么 $5$ blocks 即可 hold $80>64$ 个 inode
  - 剩下三个 block，一个放 bitmap for free inodes，一个放 bitmap for data region，一个放 superblock
  - 要 read No. 32 inode，去找 $20KB$ 对应的块
  #grid2(
    column-gutter: 4pt,
    fig("/public/assets/Courses/OS/2024-12-11-20-33-13.png"),
    fig("/public/assets/Courses/OS/2024-12-11-20-33-31.png")
  )
- 一个实际的例子 read or write `/foo/bar`，尝试说出每一步是在 read/write 哪一部分的什么数据
  #grid2(
    column-gutter: 4pt,
    fig("/public/assets/Courses/OS/2024-12-11-20-35-04.png"),
    fig("/public/assets/Courses/OS/2024-12-11-20-35-20.png")
  )

#takeaway[
  - File Descriptor
  - Link (hard and soft)
  - File System Organization
]

= Security and Protection
== Security evaluation criteria
- Trusted Computer System Evaluation Criteria(TCSEC)
  - 当前的 OS 如 windows, linux, mac 都是 C2 级别
  - 更高的是 B1, B2, B3, A1，目前没有达到的（Multics 尝试 B2 但失败了）
- ITSEC, CC, GB17859，其它几个标准

== Common concepts
- 可信基(Trusted Computing Base)
  - 为实现计算机系统安全保护的所有安全保护机制的集合，即为了做操作你需要信任的部分（比如输密码时相信键盘、程序、操作系统、CPU 等），包括软件、硬件和固件（硬件上的软件）
  - TCB in layered systems，上层依赖于下层
    #tbl(
      [Application], [Operating system], [BIOS], [Hardware/Architecture]
    )
- 攻击面(Attacking Surface)
  - 一个组件被其他组件攻击的所有方法的集合，可能来自上层、同层和底层
  - 可信基属于攻击面的一部分，比如 OS 的 Attacking Surface 包括 BIOS 和 Hardware/Architecture
- 防御纵深(Defense in-depth)
  - 为系统设置多道防线，为防御增加冗余，以进一步提高攻击难度
  - 例如，不仅仅依赖于密码，还可以加上双因素认证

== Protection - Access Control
- Authentication 认证
  - 证明用户的身份
  - 进程与用户之间如何绑定？
    - 每个进程的 PCB/cred 中均包含了 uid 字段
    - 每个进程都来自于父进程，继承了父进程的 uid
    - 用户在登录后运行的第一个进程(shell)，初始化 uid 字段
    - 在 Windows 下，窗口管理器会扮演类似 shell 的角色
- Authorization 授权
  - 决定用户能做什么
  - 如何实现？使用 Access Control Matrix (Access Control Lists, ACL)
    #tbl(
      columns: 4,
      [], [Alice], [Bob], [Carl],
      [/etc], [Read], [Read], [Read, Write],
      [/homes], [Read, Write], [Read, Write], [Read, write],
      [/usr], [None], [None], [None]
    )
  - 用户过多或文件过多时，matrix 大到不可接受。将用户（人）与角色解耦的访问控制方法: Role-Based Access Control
  - POSIX 的文件权限：分成 owner, group, other 三类，每类有 read, write, execute 三种权限
    - 即 `rwxrwxrwx`
  - 最小特权级原则：setuid 机制
    - 问题：passwd 命令如何工作？用户有权限使用 passwd 命令修改自己的密码，但保存在 `/etc/shadow` 中，用户无权访问（本质上是以文件为单位的权限管理粒度过粗）
    - 解决方法：运行 passwd 时使用 root 身份（RBAC 的思想）。在 passwd 的 inode 中增加一个 SUID 位，使得用户仅在执行该程序时才会被提权，执行完后恢复，从而将进程提权的时间降至最小
    - setuid 在 Linux 下通常用于以 root 身份运行，拥有的权限远超过必要（必要权限：读写 `/etc/passwd` 文件中的某一行；实际权限：访问整个 `/etc/passwd` 文件，且（短暂地）拥有 root 用户的权限），具有安全隐患
  - 权限控制的另一种思路 —— Capability
    - 提供细粒度控制进程的权限（初衷：解决 root 用户权限过高的问题）
    - 基本的思想是把 root 的能力拆分，分为几十个小的能力，称为 capability
    - 预先由内核定义，而不允许用户进程自定义。不允许传递，而是在创建进程的时候，与该进程相绑定，每个进程可以拥有一组能力
    - 理想美好，但实际上使用混乱
- Auditing 审计
  - 用来记录用户的操作，以便追踪和审查
- Reference monitor
  - 是实现访问控制的一种方式，主体必须通过 reference 的方式间接访问对象，Reference monitor 位于主体和对象之间进行检查
  #fig("/public/assets/Courses/OS/2024-12-24-15-21-07.png", width: 50%)
  - 引用监视器机制必须保证其不可被绕过(Non-bypassable)，即设计者必须充分考虑应用访问对象的所有可能路径，并保证所有路径都必须通过引用才能进行
  - Linux 中，应用必须通过文件描述符来访问文件，而无法直接访问磁盘上的数据或通过 inode 号来访问文件数据。文件系统此时就是引用监视器，文件描述符就是引用

== Security
- Attacks and defenses
  - Evolution #h(1fr)
    + Code injection attack
    + Code reuse attack
    + Non-control-data attack
  #fig("/public/assets/Courses/OS/2024-12-24-15-26-09.png", width: 80%)
- 代码注入攻击
  + 通过内核漏洞：篡改已有代码、注入新的代码、或者跳到用户代码
    - 内核代码注入防护：硬件支持杜绝注入、通过内核页表设置相应保护位
  + 通过内核页表来实现攻击：篡改页表去掉保护，进而篡改代码
    - 通过隔离环境保护内核页表，避免内核漏洞影响
    - 硬件支持可信执行环境
    - 实现了纵深防御
- 代码重用攻击
  - 不能注入新代码了，但可以重用已有代码(Existing code snippet, called gadget)，比如改变控制流把 gadget 串起来
    - Return-oriented programming (ROP)：通过修改栈上的返回地址，使程序跳转到已有的代码片段，从而实现攻击
    - Jump-oriented programming (JOP)：通过修改函数指针，泄露 SP
  - 防护：保护返回地址、保护函数指针（相应的硬件支持）
- 非控制数据攻击
  - 控制数据被保护后，攻击者提出非控制数据攻击，修改返回地址和函数指针以外的数据
  - 种类繁杂，难以实行统一有效保护。目前主流操作系统均*缺乏*对数据攻击的有效防护
- 总结
  #fig("/public/assets/Courses/OS/2024-12-24-15-49-51.png",width: 80%)

#takeaway[
  - Security evaluation criteria
    - TCSEC, ITSEC, CC, GB17859
  - Common concepts
    - Trusted computing base
    - Attack surface
    - Defense in-depth
  - Access Control
  - Attacks and defenses
]