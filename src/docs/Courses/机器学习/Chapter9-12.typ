#import "/src/components/TypstTemplate/lib.typ": *

#show: project.with(
  title: "机器学习",
  lang: "zh",
)

#let LL = math.op("LL")
#let SS = math.text("SS")
#let SD = math.text("SD")
#let DS = math.text("DS")
#let DD = math.text("DD")
#let dist = math.op("dist")

#info()[
  - ZJU Machine Learning 和西瓜书《机器学习 —— 周志华》的简单笔记
    - 任课老师：赵洲
    - 因为这门课讲得不是很好（高情商），西瓜书本身也写得晦涩难懂，另外机器学习的内容跟当今深度学习的内容有一定 gap，所以只简单记一下*脉络与归纳*，不会记录太复杂的公式推导
]

#counter(heading).update(8)

= EM 算法
== EM 算法的导出
- 令 $bX$ 表示已观测变量集，$bZ$ 表示隐变量集，欲对模型参数 $Th$ 做极大似然估计 (MLE)，则应最大化对数似然函数
  $ L(Th|bX, bZ) = log P(bX, bZ|Th) $
- 然而 $bZ$ 是隐变量无法直接求解（可以通过梯度下降等优化方法，但求和的项数随着隐变量数目指数级上升，因此 EM 可以看作一种非梯度优化方法），因此只能对 $bZ$ 计算期望，最大化已观测数据的对数边际似然 (log marginal likelihood)
  $
  L(Th|bX) = log P(bX|Th) = log sum_bZ P(bX, bZ|Th) \
  Th_"MLE" = argmax_Th log P(bX|Th)
  $
- EM 算法利用迭代的思想，采用两步交替的方式逐步逼近最优解
  - *E 步*，以当前参数 $Th^t$ 推断隐变量分布即后验概率分布 $P(bZ|bX,Th^t)$，计算对数似然函数 $LL(Th|bX,bZ)$ 关于 $Z$ 的期望
    $ Q(Th|Th^t) = EE_(bZ|bX, Th^t) LL(Th|bX, bZ) $
    - 注：网络上还能看到另一种记号，把 $Q$ 函数记作隐变量分布，把根据这个分布求对数似然函数的期望归结到 M 步去了
      $
      Q(bZ) = P(bZ|bX,Th) \
      Q (bz) = p(bz|bx,Th)
      $
  - *M 步*，寻找参数最大化该期望似然
    $ Th^(t+1) = argmax_Th Q(Th|Th^t) $
- EM 算法的相关证明，几个核心公式
  - 在参数 $th$ 下，所要求的对数似然与包含隐变量的对数似然的关系
    $ log p(x|th) = log p(z,x|th) - log p(z|x,th) <- p(x|th) = frac(p(z,x|th), p(z|x,th)) $
  - E 步求取期望的式子，对 $x, z$ 的联合分布
    $ Q(th, th^t) = int_z p(z|x,th^t) log(x,z|th) dif z $
  - E 步求取期望的式子，但是对 $z$
    $ H(th, th^t) = int_z p(z|x,th^t) log p(z|x,th) dif z $
  - 在证明收敛性的时候，要求 $log p(x|th^t) =< log p(x|th^(t+1))$
    - 由于 $th^(t+1)$ 的定义，$Q(th^t,th^t) =< Q(th^(t+1),th^t)$，只需证 $H(th^t,th^t) >= H(th^(t+1),th^t)$
    - 而这可以转化为 KL 散度的非负性

== EM 算法用于混合模型
- 如果观测数据 $bx$ 是由 $K$ 个某种分布生成，$th$ 刻画了这些分布的参数，利用混合模型描述这种情况
  $ p(bx|th) = sumkK p(z_k|th) p(bx|z_k, th) $
  - $p(z_k|th)$ 表示结果来自第 $k$ 个分布的概率，满足
    $ sumkK p(z_k|th) = 1 $
    - 本质上，就是因为这玩意儿不知道，所以只能以 EM 算法用隐变量和隐变量的分布来表示
  - 但是需要注意 $p(z_k|bx,th^t) != p(z_k|th^t)$
    - 前者是 E 步需要估计的核心量 —— 隐变量的后验概率；后者则是我们求值的目标（最后要求解的参数，每一步迭代更新逼近）
  - 将 $p(z_k|th), ~p(bx|z_k,th)$ 替换为不同模型，如高斯分布、多项式分布等，就得到 EM 算法在混合模型中的不同变种
- $Q$ 函数表示为
  $ Q(th,th^t) = EE_(z|bx, th^t) log P(bx,z|th^t) = sumkK log p(bx,z_k|th^t) p(z_k|bx,th^t) $
  - 考试的时候，不要自己推这个恶心公式了，直接代吧
  - 但实际情况中肯定不止是一个观测
    - 于是上述公式里面 $bx, z$ 都得带个下标 $j=1,dots,N$，似然函数、极大似然估计等玩意儿闻着味就来了，不要被这些搞晕了，不管这些，对上式根据 $j=1 wave N$ 求个和
      $ Q(th,th^t) = sumkK sumjN log p (bx_j,z_k|th^t) p(z_k|bx_j,th^t) $
- 对于 GMM，推导如下
  - E 步
    $
    p(bx_j,z_k|th^t) = p(z_k|th^t) p(bx_j|z_k,th^t) = al_k N(bx_j|mu_k, Si_k) \
    p(z_k|bx_j,th^t) = frac(p(bx_j,z_k|th^t), p(bx_j|th^t)) = frac(p(bx_j,z_k|th^t), sumkK p(z_k|th^t) p(bx_j|z_k,th^t)) = frac(al_k N(bx_j|mu_k, Si_k), sumkK al_k N(bx_j|mu_k, Si_k)) eq.delta ga_(jk) \
    Q(th,th^t) = sumkK {log al_k sumjN ga_(jk) + sumjN [log N(bx_j|mu_k, Si_k) ga_(jk)]}
    $
  - M 步，对 $mu_k, Si_k$ 没有约束，这里从略，只看 $al_k$ 的更新
    $ th^(t+1) = argmax_th Q(th,th^t) $
    - 由拉格朗日乘子法，只考虑跟 $al_k$ 有关的项
      $ L(al_k, la) = log al_k sumjN ga_(jk) + la(sumkK al_k - 1) $
    - 对 $al_k$ 求导
      $ frac(pa L, pa al_k) = sumjN ga_(jk) dot frac(1, al_k) + la = 0 $
    - 结合 $sumkK al_k = 1, sumkK ga_(jk) = 1$，得到 $la = -N$，并代回得到
      $ al_k = frac(sumjN ga_(jk), N) $
    - 对 GMM，还是背一下它的公式，另外两个的公式为
      $
      mu_k = frac(sumjN ga_(jk) x_i, sumjN ga_(jk)) \
      si_k^2 = frac(sumjN ga_(jk) (x_i-mu_k)(x_i-mu_k)^T, sumjN ga_(jk))
      $
- 对三硬币模型，推导如下
  - 如果用 $p(x_j = 1|th^t)$ 这种记号，表达上会显得非常丑陋，可以这样写
    $
    p(x_j|th^t) = pi p^(x_j) (1-p)^(1-x_j) + (1-pi) q^(x_j) (1-q)^(1-x_j)
    $
  - 对数似然函数
    $
    LL(Th|bX,bZ) = sumjN log sumkK p(x_j,z_k|th) = sumjN log [pi p^(y_i) (1-p)^(1-y_i)]^(z_k) [(1-pi) q^(y_i) (1-q)^(1-y_i)]^(1-z_k) \
    $
  - 但好像还是没法避免 $p(z_1|th^t), p(z_2|th^t)$ 这种记号，记结果来自硬币 B 的后验概率为 $mu$（A 的结果为正的概率）
    $
    mu_j = p(z_1|x_j,th^t) = frac(pi p^(x_j) (1-p)^(1-x_j), pi p^(x_j) (1-p)^(1-x_j) + (1-pi) q^(x_j) (1-q)^(1-x_j))
    $
  - E 步，推导得到 $Q$ 函数
    $
    Q(th|th^t) = EE_(z|bx, th^t) LL(Th|bX,bZ) = sumjN mu_j log pi p^(x_j) (1-p)^(1-x_j) + (1-mu_j) log (1-pi) q^(x_j) (1-q)^(1-x_j)
    $
  - M 步，上式对 $pi, p, q$ 求偏导，不涉及约束
- 对语料库模型，推导如下
  - 如果已知文档 $d$ 来自语料库 $c_d=k$，则满足多项分布
  $
  p(d|c_d=k, th^t) = frac((sum_w^W T_(d w))!, Pi_w^W T_(d w) !) Pi_w^W mu_(k w)^(T_(d w))
  $
  - E 步，后验概率估计
  $
  ga_(d k) = p(c_d=k|d, th^t) &= frac(p(d|c_d=k, th^t) p(c_d=k|th^t),p(d|th^t)) \
  &= frac(p(d|c_d=k, th^t) p(c_d=k|th^t), sum_(i=1)^K  p(d|c_d=i, th^t) p(c_d=i|th^t)) \
  &= frac(pi_k Pi_w^W mu_(k w)^(T_(d w)), sum_(i=1)^K pi_i Pi_w^W mu_(i w)^(T_(d w)))
  $
  - M 步，两个都有约束都要拉格朗日乘子法，略

= 集成学习
- 集成学习(ensemble learning)通过构建并结合多个学习器来提升性能
  - 个体学习器的“准确性”和“多样性”本身就存在冲突，如何产生“好而不同”的个体学习器是集成学习研究的核心
  - 集成学习大致可分为两大类: 串行(Boosting) vs 并行(Bagging)

== Boosting
- Boosting: 个体学习器存在强依赖关系；串行生成；每次调整训练数据的样本分布。Boosting 最著名的代表是 Adaboost
- 问题的提出：只要找到一个比随机猜测略好的弱学习算法就可以直接将其提升为强学习算法，而不必直接去找很难获得的强学习算法。两个核心问题：(1) 怎样获得不同的弱分类器？(2) 怎样组合弱分类器？
- 对于前者，有以下几种方法：
  - 使用不同的弱学习算法得到不同基本学习器（参数估计、非参数估计……）
  - 使用相同的弱学习算法，但用不同的参数（K-Means 不同的 $K$，神经网络不同的隐含层……）
  - 相同输入对象的不同表示凸显事物不同的特征
  - 使用不同的训练集 $->$ 装袋(bagging)，提升(boosting)
- 对 Adaboost，每一轮如何改变训练数据的权值或概率分布？被前一轮弱分类器错误分类样本的权值提高（数据集每个样本都赋予一个权值）；如何将弱分类器组合成一个强分类器？加权多数表决，加大分类误差率小的弱分类器的权值
#algo(title: [*Algorithm* of *AdaBoost*])[
  - 输入：二分类训练数据集 $T = {(x1,y1), ..., (xn,yn)}, x_i in cal(X) subset RR^n, yi in cal(Y) = {-1,+1}$
  - 输出：最终分类器 $G(x)$
  + 1. 初始化训练数据的权值分布 $D_1 = (w_11, ..., w_(1N)), w_(1i) = 1/N$
  + 2. 对 $m = 1,2,...,M$
    + a. 在权值 $D_m$ 下训练数据集，得到弱分类器 $G_m (x) : cal(X) -> {-1,+1}$
    + b. 计算 $G_m (x)$ 的训练误差：$e_m = sum_(i=1)^N P(G_m (x_i) != y_i) = sum_(i=1)^N w_(m i) I(G_m (x_i) != y_i)$
    + c. 计算弱分类器 $G_m (x)$ 的系数：$al_m = 1/2 log (1-e_m)/e_m$ #redt[$=>$ 思考为什么]
    + d. 更新训练数据集的权值分布 $D_(m+1) = (w_(m+1 1), ..., w_(m+1 N))$
      $ w_(m+1, i) = frac(w_(m i) exp(-al_m y_i G_m (x_i)), sum_(j=1)^N w_(m j) exp(-al_m y_j G_m (x_j))) $
  + 3. 构建弱分类器的线性组合，并以其符号作为最终结果 $G(x) = "sign"(sum_(m=1)^M al_m G_m (x))$
]
- 说明
  + 步骤 c，$al_m = 1/2 log (1-e_m)/e_m$，当 $e_m =< 1/2$ 时，$al_m >= 0$。即误分类小于一半（优于随机）时，对最终分类器贡献为正
  + 步骤 d，更新后的权值分布，误分类样本的权值相对正确样本增大 $exp(2al_m)=frac(1-e_m, e_m)$ 倍，有点类似学习率衰减
- Adaboost 的 $al$ 是怎么来的？为了保证最终分类器的训练误差界：
  $ 1/N sum_(i=1)^N I(G(x_i)!=yi) =< 1/N sum_(i=1)^N exp(-y_i f(x_i)) = underbrace(Pi_(m=1)^M Z_m = Pi_(i=1)^M 2 sqrt(e_m (1-e_m)), ["use equation of" al]) =< exp(-2 sum_(i=1)^M (1/2-e_m)^2) $
- \*Adaboost 本质是个以“加法模型”为模型，以“指数损失函数”为损失函数，以“前向分步算法的二分类学习算法”为学习算法的方法。可以看到它的学习器是不断“提升”的，互相之间有强依赖关系
- *从偏差-方差的角度*理解：降低偏差，可对泛化性能相当弱的学习器构造出很强的集成

== Bagging 与随机森林
- 个体学习器不存在强依赖关系、并行化生成、自助采样法
- 我们可以用自助采样法得到 $T$ 个含 $m$ 个训练样本的采样集，基于每个采样集训练出一个基学习器，这就是 Bagging 的基本流程
- 在对预测输出进行结合时，通常对分类任务使用简单投票法（若收到同样票数，可以随机选一个，也可以进一步按照置信度选择），对回归任务使用简单平均法
- Bagging 的时间复杂度非常低，假定基学习器的计算复杂度为 $O(m)$，采样与投票/平均过程的复杂度为 $O(s)$，则 bagging 的复杂度大致为 $T(O(m)+O(s))$。由于 $O(s)$ 很小且 $T$ 是不大的常数，通常训练一个bagging 集成与直接使用基学习器的复杂度同阶
- Bagging 的泛化误差 —— 包外估计
- *从偏差-方差的角度理解*：降低方差，通过降低模型的方差，在不剪枝的决策树、神经网络等易受样本影响的学习器上效果更好
- 随机森林 (Random Forest, RF) 是 bagging 的一个扩展变种
  - 采样的随机性、属性选择的随机性
  - 略

== 结合策略
  - 学习器结合可能会从三个方面带来好处
    + 从统计方面来看：学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能。此时单学习器可能因误选导致泛化性能不佳，而结合多个学习器可以减小这种风险
    + 从计算方面来看：多个学习器降低了陷入局部极小的风险
    + 从表示方面来看：结合多个学习器，相应的假设空间可能更大，可以更好地逼近真实的潜在关系
  - 常见的结合策略
    + 平均法，简单平均和加权平均。前者是后者的特例，但后者由于过拟合而不一定总优于前者
    + 投票法
      - 绝对多数投票法提供了不到一半票数时的拒绝预测选项；但若一定要求给出预测时，退化为相对多数投票法；此外还有加权投票法
      - 类标记与类概率（硬投票与软投票）
      - 一些学习器能在输出类别标记的同时给出分类置信度，此置信度可以转化为类概率使用。但该值有时需要进行规范化（如 SVM 的分类间隔值需要以某种方式转化为类概率）
    + 学习法
      - 通过另一个学习器来结合，于是分出个体学习器（初级学习器）和元学习器（次级学习器）两个概念
      - stacking 是学习法的典型代表

== 多样性
=== 误差-分歧分解
- 对回归任务，定义个体学习器与集成的分歧、个体学习器与真值的平方误差、集成学习器与真值的平方误差（都是针对某一个 $x$ 而言）
  $
  oA(h|x) = sum_(i=1)^T w_i (h_i (x) - H(x)) \
  oE(h|x) = sum_(i=1)^T w_i (f(x) - H(x))^2 \
  E(H|x) = (H(x) - f(x))^2
  $
- 能够得到式子
  $ E = oE - oA $
  - 这个式子显示：个体学习器精确性越高（跟真值的误差越小）、多样性越大（跟集成的分歧越大），则集成效果越好
  - 我们前面用各种方法试图更好地集成个体学习器，而这里给出了一个直接的描述，那可否直接回归 $oE - oA$ 呢？
  - 答案是不行，因为现实任务中很难对 $oE - oA$ 进行优化，这是因为
    + 它们定义在整个样本空间上
    + $oA$ 不是一个可直接操作的多样性度量
    + 上面的推导过程只适用于回归学习，难以直接推广到分类学习任务上去
=== 多样性度量
- 既然 $oA$ 不好操作，我们就考虑其它多样性度量（差异性度量）
- 跟性能度量一样，对于二分类问题，构建分类器 $h_i$ 与 $h_j$ 的预测结果联立表(contingency table)
  #tbl(
    columns: 3,
    [], [$h_i = +1$], [$h_i = -1$],
    [$h_j = +1$], [$a$], [$c$],
    [$h_j = -1$], [$b$], [$d$]
  )
  $ a+b+c+d=m $
  - 不合度量(disagreement Measure)
    $ "dis"_ij = frac(b+c, m) $
  - 相关系数(correlation)
    $ rho_ij = frac(a d - b c, sqrt((a+b)(a+c)(c+d)(b+d))) $
  - $Q$ 统计量
    $ Q_ij = frac(a d - b c, a d + b c) $
    - 与相关系数的关系是：符号相同且 $Q_ij$ 更小
  - $kappa$ 统计量
    $
    kappa = frac(p_1 - p_2, 1 - p_2) \
    p_1 = frac(a+d, m), p_2 = frac((a+b)(a+c) + (c+d)(b+d), m^2)
    $
    - $kappa$ 在 $h_i, h_j$ 完全一致时取 $1$，在偶然一致时取 $0$，通常希望取负值（达成一致的概率低于偶然）
    - 将个体学习器两两组合，把二者的 $kappa$ 值跟平均误差绘制成图，可以得到 $kappa-"误差图"$
=== 多样性增强
- 常见方法
  - 数据样本扰动
    - 通常基于采样法，如 Bagging 中的自助采样法、Adaboost 中的序列采样
    - 数据样本扰动对“不稳定基学习器”很有效
      - 对数据样本的扰动敏感的基学习器（不稳定基学习器）：决策树，神经网络等
      - 对数据样本的扰动不敏感的基学习器（稳定基学习器）：线性学习器，支持向量机，朴素贝叶斯，k近邻等
  - 输入属性扰动：随机子空间（属性子集）算法
  - 输出表示扰动
    - 翻转法 (Flipping Output)：随机改变一些训练样本的标记（太坏了 #emoji.face.devil）
    - 输出调剂法 (Output Smearing)：如将分类输出转化为回归输出
    - ECOC 法：将多分类问题转化为多个二分类问题
  - 算法参数扰动
    - 负相关法：典型代表如神经网络的正则化
    - 随机设置参数
      - 使用单一学期器时通常使用交叉验证方法确定参数值，事实上已使用不同参数训练多个学习器，只不过仅使用其中一个；由此也能看出集成学习的实际开销并不比单一学习器大很多
  - 结合使用上述多种扰动方法，例如随机森林同时使用了数据样本扰动和输入属性扰动

= 聚类
- 聚类任务
  - 将数据样本划分为若干个通常不相交的“簇”(cluster)。不相交的簇 —— 硬聚类；可相交的簇 —— 软聚类

== 性能度量
- 基本想法：“簇内相似度”(intra-cluster similarity)高，且“簇间相似度”(inter-cluster similarity)低
- *外部指标 (external index)*：将聚类结果与某个“参考模型” (reference model) 进行比较，如 Jaccard 系数，FM 指数，Rand 指数
  - 数据集 $D={x1,x2,x_m}$，聚类给出的簇划分为 $cC={C_1,C_2,dots,C_k}$，参考模型给出的簇划分为 $cC^*={C_1^*,C_2^*,dots,C_k^*}$，$la_i$ 与 $la^*_i$ 分别为在两种模型下的簇标记。将样本两两配对，定义
    $
    a=abs(SS), SS={(x_i,x_j) | la_i=la_j, la_i^*=la_j^*, i < j} \
    b=abs(SD), SD={(x_i,x_j) | la_i=la_j, la_i^*!=la_j^*, i < j} \
    c=abs(DS), DS={(x_i,x_j) | la_i!=la_j, la_i^*=la_j^*, i < j} \
    d=abs(DD), DD={(x_i,x_j) | la_i!=la_j, la_i^*!=la_j^*, i < j} \
    a + b + c + d = m(m-1)/2 \
    "JC" = frac(a,a+b+c), ~ "RI" = frac(2(a+d),m(m-1)), ~ "FMI" = sqrt(frac(a,a+b) dot frac(a,a+c))
    $
  - 其中 $a,d$ 是 “好” 的，$b,c$ 是 “坏” 的。这些外部指标都是越大越好（跟参考模型越接近）
- *内部指标 (internal index)*：直接考察聚类结果，无参考模型，如 DB 指数，Dunn 指数
  - DB 指数对每个簇 $i$，考察簇内距离大且跟簇 $i$ 接近（也就是最差）的簇 $j$，对所有这样的 $i$ 取平均
    - 它衡量簇的分离度和紧密度的比值，越小越好（助记：DB(bad) $->$ 越小越好）
  - Dunn 指数考察簇间最小距离最小且所有簇的直径最大
    - 它衡量簇的紧密度和分离度的比值，越大越好
  $
  avg(C) = frac(2,abs(C)(abs(C)-1)) sum_(1=<i<j=<abs(C)) "dist"(C_i,C_j) ~~~ "聚类内平均距离" \
  "diam"(C) = max_(1=<i=<abs(C)) max_(x,y in C_i) "dist"(x,y) ~~~ "聚类内最大距离" \
  d_min (C_i, C_j) = min_(x in C_i, y in C_j) "dist"(x,y) ~~~ "聚类间最小距离" \
  d_"cen" (C_i, C_j) = "dist"(bmu_i, bmu_j) ~~~ "聚类中心距离"
  $ <->
  $
  "DB指数" ~ &"DBI" = 1/k sum_(i=1)^k max_(j!=i) frac(avg(C_i) + avg(C_j), d_"cen" (C_i, C_j)) \
  "Dunn指数" ~ &"DI" = frac(min_(1=<i < j=<k) d_min (C_i, C_j), max_(1=<l=<k) "diam"(C_l))
  $

== 距离计算
- 距离度量需满足的基本性质：
  + 非负性，很好理解
  + 同一性，距离为零当且仅当两点相同
  + 对称性，很好理解
  + 直递性，就是三角不等式
- 常用距离函数：闵可夫斯基距离
  $ "dist"(x_i, x_j) = (sum_(u=1)^n abs(x_i^u - x_j^u)^p)^(1/p) $
  - $p=2$ 为欧氏距离，$p=1$ 为曼哈顿距离，$p=infty$ 为切比雪夫距离
  - 当 $p >= 1$ 时，满足直递性；当 $0 =< p < 1$ 时不满足

== 聚类算法
=== 原型聚类
- 原型=簇中心，即有簇中心的聚类方法，亦称“基于原型的聚类”(prototype-based clustering)
- 假设：聚类结构能通过一组原型刻画
- 过程：先对原型初始化，然后对原型进行迭代更新求解
- 代表：*Kmeans 聚类*，*学习向量量化(LVQ)*，*高斯混合聚类*
  - *Kmeans：*
    + 随机选取k个样本点作为簇中心
    + 将其他样本点根据其与簇中心的距离，划分给最近的簇
    + 更新各簇的均值向量，将其作为新的簇中心（若不以均值向量为中心，以距离它最近的样本点为中心，得到 k-medoids算法）
    + 若所有簇中心未发生改变，则停止；否则执行 Step 2
  - *学习向量量化*（应用于有监督聚类）
    #fig("/public/assets/Courses/ML/2024-12-03-17-01-06.png", width:60%)
    - 简单来说就是用一组原型向量像样本点一样在空间中移动，用类似神经网络的方法训练这些原型向量
  - *高斯混合聚类*(GMM)：采用高斯概率分布来表达聚类原型，簇中心=均值，簇半径=方差 $-->$ *EM 算法*
    - $n$ 维样本空间中随机向量 $bx$ 服从高斯分布，其概率密度函数为
      $ p(bx|bmu, bold(Si)) = frac(1, (2pi)^(n/2) abs(bold(Si))^(1/2)) exp(-1/2 (bx-bmu)^T bold(Si)^(-1) (bx-bmu)) $
    #algo(title: [*Algorithm *: EM of GMM])[
      - 假设样本根据 $k$ 个高斯分布生成
        $ p_M (bx) = sum_(i=1)^k al_i p_i (bx|bmu_i, bold(Si)_i) $
      + 初始化高斯混合分布的参数 $Th = {(al_i, bmu_i, bold(Si)_i)|1=<i=<k}$
      + 然后开始迭代，每一轮先做 E 步估计后验概率
      - $ ga_(ji) = p_M (z_j=i|bx_j,Th) $
      + 然后 M 步最大化参数，直到收敛或达到最大迭代次数
    ]
=== 密度聚类
- 划成多个等价类，未必有簇中心，亦称“基于密度的聚类” (density-based clustering)
- 假设：聚类结构能通过样本分布的紧密程度确定
- 过程：从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇
- 代表：*DBSCAN*, OPTICS, DENCLUE
  - DBSCAN
    #fig("/public/assets/Courses/ML/2024-12-03-17-05-15.png",width:60%)
    - *密度直达*通常不满足对称性；*密度可达*也是，但满足直递性；*密度相连*满足对称性
    - DBSCAN 的“簇”定义为 —— 由密度可达关系导出的最大密度相连样本集合
    - DBSCAN 算法首先遍历所有顶点，确定核心对象集合 $Om$，之后每次从中选取一个“种子”来生成聚类簇，直到 $Om$ 为空
    #fig("/public/assets/Courses/ML/2024-12-30-16-16-23.png", width:50%)

=== 层次聚类
- 聚类效果跟抽象的粒度有关，形成多层次聚类
- 假设：能够产生不同粒度的聚类结果
- 过程：在不同层次对数据集进行划分，从而形成树形的聚类结构
- 代表：*AGNES*（自底向上），DIANA（自顶向下）
  - AGNES
    + 首先构建距离矩阵，称为 $D$
    + 将每个样本点作为一个簇
    + 每次从矩阵中找最小值，合并最近的两个簇，将它们从矩阵中去掉，加入新的簇，并且更新矩阵中与其它簇的距离
    + 若所有样本点都存在于给定 $k$ 值个簇中，则停止；否则转到 Step3
    - 根据距离度量选择 $d_"min", d_"max", d_"avg"$ 的不同，AGNES 算法被称为*单连接*、*全连接*、*均连接*算法

= 降维与度量学习
- k-近邻学习用作引入
  - 投票法、平均法
  - 懒惰学习的代表
  - 最近邻分离器的泛化错误率不会超过贝叶斯最优分类器错误率的两倍 $P("err") =< 2 times (1 - P(c^*|bx))$
    - 推导，错误率就是 $1$ 减去给 $bx$ 和其最近邻 $bz$ 分配相同的类 $c$，然后估计大小
  - 维数灾难：随着维数的增加，样本空间中的样本密度会迅速减小，导致 k-近邻学习的错误率迅速增大 $->$ 需要进行降维

== 低维嵌入
- *多维缩放方法*(Multiple Dimensional Scaling, *MDS*)
  - 旨在寻找一个低维子空间，使得*距离*和样本原有距离近似不变
  - 主要思路：内积保距，从距离矩阵到内积矩阵，推导不难，可以记一下
    $ b_ij = -1/2(dist_ij^2 - dist_(i dot)^2 - dist_(dot j)^2 + dist_(dot dot)^2) $
  - 然后寻找低维子空间尽量保持样本内积不变。方法就是*特征值分解*
- *PCA*
  #algo(title: [*Algorithm*: PCA])[
    - 输入：给定矩阵 $X^((0)) = [~~]_(N times m)$（$N$ 个维度为 $m$ 的样本）
    + 按行求和得到均值 $overline(X)$，进行去中心化得到 $X^((1)) = X^((0)) - overline(X)$
    + 计算协方差矩阵 $R$ 的对角线值 $s_ii = 1/(N-1) X^((1)) X^((1)T)$，然后对 $X^((1))$ 做规范化 $x^((2))_ij = (x_ij - overline(x)_i) / sqrt(s_ii)$，得到 $X^((2))$
    + 求样本相关矩阵 $R=X^((2)) X^((2)T)$
    + 对这个矩阵求特征值分解，特征值排序即为*主成分的方差*，可以求方差贡献率
    + 求解特征向量并单位化即为*主成分*，它们排列起来就是*投影矩阵*
  ]
  - SVD 分解用于 PCA
    #algo(title: [*Algorithm*: PCA (SVD Ver.)])[
      - 输入：给定矩阵 $X^((0)) = [~~]_(N times m)$（$N$ 个维度为 $m$ 的样本）
      + 按行求和得到均值 $overline(X)$，进行去中心化得到 $X^((1)) = X^((0)) - overline(X)$
      + 构建新的矩阵 $X^((2))=frac(1,sqrt(n-1)) X^((1))$
      + 对 $X^((2))$ 做 SVD 分解和截断，其中 $U in RR^(N times m), S in RR^(m times m), V in RR^(m times m), U_k in RR^(N times k), S_k in RR^(k times k), V_k in RR^(m times k)$
      - $ X^((2)) = U S V^T approx U_k S_k V_k^T $
      + 取 $V_k$ 作为投影矩阵，得到 $Y = X^((1)) V_k$
    ]
- 核化线性降维 *KPCA*，引入非线性性
  - 这里的核函数跟 SVM 那边的是一个意思，主要思路就是把数据从样本空间映射到高维特征空间，在高维空间中的线性操作对应于原始空间中的非线性操作。相当于是先把 $X$ 升维到更好分解的高维空间 $Z$，然后再做 PCA 降维到低维空间 $Y$
  #algo(title: [*Algorithm*: KPCA])[
    + 选择核函数 $kappa(dot, dot)$，应用于 $X$ 得到 $K$
    + 对 $K$ 进行中心化
    + 对 $K$ 进行特征值分解，选择最大的 $d$ 个特征值对应的特征向量，组成投影矩阵 $A in RR^(N times d)$
    + 对 $X$ 进行投影 $Y = A^T X$，通过下式得到投影后坐标
    - $ vec(y_1, y_2, dots.v, y_d) = sumiN vec(al_i^1 kappa(bx_i, bx), al_i^2 kappa(bx_i, bx), dots.v,  al_i^d kappa(bx_i, bx)) $
  ]
- 为获得投影点坐标，PCA 和 KPCA 需要对所有样本求和，计算开销较大

== 流形学习
- 参考 #link("https://zhuanlan.zhihu.com/p/40214106")[知乎文章]，比西瓜书讲得好多了
- 什么是流形？
  - 流形 (manifold) 是几何中的一个概念，它是高维空间中的几何结构，即空间中的点构成的集合。可以简单的将流形理解成二维空间的曲线、三维空间的曲面在更高维空间的推广。它在局部具有欧氏空间的性质，能用欧氏距离进行计算
  - 那么，我们可以借此把样本数据空间映射到流形空间，虽然不知道这个流形空间长什么样，但可能可以映射到比样本空间更低维的空间，从而实现降维
- 什么是流形学习？
  - 流形学习 (manifold learning) 假设数据在高维空间的分布位于某一更低维的流形上，基于这个假设来进行数据的分析
  - 对于降维，要保证降维之后的数据同样满足与高维空间流形有关的几何约束关系
  - 除此之外，流形学习还可以用实现聚类，分类以及回归算法
- *等距映射*(Isometric Mapping, *Isomap*)
  - 使用了微分几何中测地线的思想，希望数据在向低维空间映射之后能够保持流形上的*测地线距离*。直观打个比方就是将 3D 的地球仪投影到 2D 的平面地图上，国家之间的*距离信息得以保留*
  - 计算任意两个样本之间的测地距离（利用局部与欧氏空间同胚的性质，用欧氏意义下的 k 近邻），然后根据这个距离构造距离矩阵。然后通过距离矩阵求解优化问题（利用 MDS，还记得 MDS 的核心吗？内积*保距*！）完成数据的降维，降维之后的数据保留了原始数据点之间的距离信息
  #algo(title: [*Algorithm*: ISOMAP])[
    + 确定每个样本 $x_i$ 的 k 近邻
    + 将 $x_i$ 与其 k 个近邻点之间的距离设置为欧氏距离，与其他点的距离设置为无穷大
    + 调用最短路径算法计算任意两样本点之间的距离
    + 将这个距离作为 MDS 算法的输入
  ]
  - 需注意的是，Isomap 仅得到了训练样本在低维空间的坐标，还无法直接映射新样本。这个问题的常用解决方案，是将训练样本的高维空间坐标作为输入、低维空间坐标作为输出，训练一个回归学习器来对新样本的低维空间坐标进行预测
- *局部线性嵌入*(Locally Linear Embedding, *LLE*)
  - 核心思想是每个样本点都可以由与它相邻的多个点的线性组合（体现了局部线性，且将变动限制在局部）来近似重构，这相当于用分段的线性面片近似代替复杂的几何形状，样本投影到低维空间之后要保持这种线性重构关系，即有相同的重构系数
  - 首先为每个样本找到近邻下标集合，计算出高维空间中的*重构系数*(close-form)，用同样的系数在低维空间中求解样本的低维投影
  #algo(title: [*Algorithm*: LLE])[
    + 确定每个样本 $x_i$ 的 k 近邻
    + 若 $j in Q_i$，则从下式计算重构系数 $w_ij$，否则 $w_ij = 0$
    - $ min_(w_1,w_2,dots,w_n) sumiN norm(bx_i - sum_(j in Q_i) w_ij bx_j)_2^2 \ s.t. sum_(j in Q_i) w_ij = 1 $
    + 是在低维空间中保持 $w_ij$ 不变，求解下式
    - $ min_(bz_1,bz_2,dots,bz_m) sumiN norm(bz_i - sum_(j in Q_i) w_ij bz_j)_2^2 $
      - 实际上可以转化为从权值矩阵 $W$ 计算矩阵 $M$ 然后做特征值分解，取最小的 $d'$ 个特征值对应的特征向量组成 $Z^T$
        $
        "let"~~ Z = (bz_1, bz_2, dots, bz_m) in RR^(d' times N), M = (I - W)^T (I - W), \
        "optimize"~~ min_(Z) tr(Z M Z^T) ~~~ s.t. ~ Z Z^T = I
        $
  ]

== 度量学习
- 降维、聚类等算法需要一个合适的低维空间下的距离度量，需要事先给定
- 希望能够直接学出合适的度量，用马氏距离(Mahalanobis distance)
  $ "dist"_"mah" (bx_i,bx_j) = (bx_i - bx_j)^T M (bx_i - bx_j) = norm(bx_i-bx_j)_M $
  - $M$ 为学习目标，为了保持距离非负且对称，它必须是（半）正定矩阵
- $M$ 怎么学习？要有个合适的目标函数
- 目标一：*结合具体分类器的性能*
  - 例如，在近邻成分分析(Neighbourhood Component Analysis, NCA)中，可以将多数投票法替换为（基于距离的）概率投票法，从而将 $M$ 直接嵌入到近邻分类器的评价指标中，通过优化该性能指标相应地求得 $M$
    $ p_ij = frac(exp(-norm(bx_i-bx_j)_M^2), sum_l exp(-norm(bx_i-bx_l)_M^2)) $
- 目标二：*结合领域知识*
  - 例如，若已知某些样本相似与不相似，则可定义 “必连” 约束集合 $cM$ 和 “勿连” 约束集合 $cC$，则可求解下述凸优化问题得到 $M$
    $
    min_M sum_(bx_i,bx_j in cM) norm(bx_i-bx_j)_M^2 \
    s.t. sum_(bx_i,bx_j in cC) norm(bx_i-bx_j)_M^2 >= 1 \
    M succ.eq 0
    $
- 另外，如果学出的度量矩阵 $M$ 是一个低秩矩阵，则通过对其进行特征值分解并找出正交基（数目等于 $M$ 的秩，小于原属性数目 $d$），就可以从度量学习的结果衍生出一个降维矩阵 $P in RR^(d times "rank"(M))$

#note(caption: "总结降维方法")[
  + LDA 是*有监督线性降维*方法，通过最大化类间距离和最小化类内距离找到最优投影方向
  + MDS 方法是*线性降维*方法，从距离矩阵利用骚操作直接算出内积矩阵，然后特征值分解降维尽量使内积不变
  + PCA 是*线性降维*方法，通过特征值分解找到使方差最大的主成分，利用 kernel 方法引入非线性
  + ISOMAP 是*非线性降维*方法，通过测地线距离计算高维空间的距离关系，利用 MDS 保距的特点降维
    - 其非线性，个人认为来自于欧氏距离模拟测地线距离那一步
  + LLE 是*非线性降维*方法，通过局部线性重构关系降维
    - 其非线性，个人认为来自于重构系数的局部性，不在近邻集合的直接设为 $0$（有点像 ReLU）
  + 也可以基于度量矩阵的低秩性，通过特征值分解得到降维矩阵
  - LDA 和 PCA 需要会算
]
