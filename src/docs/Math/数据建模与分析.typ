#import "/src/components/TypstTemplate/lib.typ": *

#show: project.with(
  title: "数据建模与分析笔记",
  lang: "zh",
)

= 机器学习简介
- 没记

= 感知机
- 没记

= k-近邻算法
- 没记

= 朴素贝叶斯分类器
- 没记

= 决策树
- 没记

= Logistic回归和最大熵模型
- 关于特征函数与期望
特征函数$f(x, y)$关于经验分布 $hat(P)(X, Y)$ 的期望值:$ E_(hat(P)) (f) = sum_(x, y)hat(P) (x,y)f(x,y) $
关于联合分布 $P(X,Y)$ 的期望值：$ E_(P) (f) = sum_(x, y)P (x,y)f(x,y) = sum_(x,y)P(x)P(y|x)f(x,y) $
用 $hat(P)(x)$ 代替（or 估计）未知的 $P(x)$，就得到书上的公式。

- 关于条件熵
定义在条件概率分布 $P(Y|X)$ 上的条件熵： $ underbrace(H(P(Y|X)), H(P)) = -sum_(x,y)hat(P)(x)P(y|x) log P(y|x) $
疑惑这里跟以往的 $H(P)=-sum P(x) log P(x)$ 不一样。

注意条件熵的定义是，在一个变量$X$的条件下（变量$X$的每个值都会取），另一个变量$Y$熵对$X$的期望，即：$ H(Y|X)=sum_(x,y) p(x) H(Y|X=x) $用经验分布 $hat(P)(x)$ 估计 $P(x)$，就得到书上的公式。

- 最大熵模型的距离，讲的是更复杂的条件熵，距离用的是简单的熵。
- 最大熵模型把内部极小化问题做好了，实际运用时考虑外部极大化问题。
- 对偶函数的极大化等价于最大熵模型的极大似然估计：
  - 这里对数似然函数少个无关紧要的常数项乘积
- 拟牛顿法看懵了
  - P43 最后又用了个近似？
- 改进的迭代尺度法，通过对 $A(delta|w)$ 的进一步放缩，把 $f_i(x,y)$ 从 exp 中独立出来，方便后面求导

- Softmax 分类模型是最大熵模型的一个特例，LR 是 Softmax 模型在二分类时的特例（当然也可以说 LR 不止二分类）

= 支持向量机SVM
- PPT 51 页缺少了由两个不等式约束导出的两个对偶互补条件
$
alpha_i (1-xi_i - y_i (w dot x_i+b)) = 0\
-mu_i xi_i = 0
$
$alpha_j$ 为 0 时，无法求出 $b$；$alpha_j$ 为 $C$ 时，推出 $mu_j = 0$，导致无法确定 $xi_j$ 为 $0$，因而也无法求出 $b$。

= EM 算法
- #link("https://blog.csdn.net/weixin_41566471/article/details/106219019")[三硬币模型的推导（有点小错）]

= 聚类方法
- 欧式距离和夹角余弦的推广 $d_(i j)=(sum_(k=1)^m |x_(k i)-x_(k j)|^2)^(1/2) "or" (sum_(k=1)^m x_(k i) x_(k j)) / (norm(x_i) norm(x_j))$
  - $=> d_(i j)=((x_i-x_j)^T M (x_i-x_j))^(1/2) "or" d_(i j)=x_i^T M x_j$，令 $M=L^T L$
  - 这个 $M$ 是需要学习出来的，相当与是对所有数据先进行一个线性变换，再计算欧式距离或夹角余弦并聚类
  - 这样，两个数据之间的距离考虑到了整个数据集性质，效果会更好，这就是“度量学习”。
- 关于机器学习中的协方差矩阵和数学中的协方差矩阵，#link("https://zhuanlan.zhihu.com/p/422028586")[对机器学习中的协方差矩阵还是傻傻的搞不清楚？这次我终于捋明白了！]
$ "聚类"=cases(
  "层次聚类"=cases(
    "聚合聚类",
    "分裂聚类"
  )
) $

= 谱聚类
- 应该算是拓展？书上没有
- 拉普拉斯矩阵一定有一个 0 特征值，对应的特征向量是全 1 向量，
  - 它的重数就是图的连通分量数，但在实际情况中，可能会小于（因为图的联通分量之间可能还有*很弱的链接*），为此采用前 k 个最小特征值
  - 本质上就是从原始空间转到与谱定理相关的特征空间，在特征空间中进行聚类
  - 而归一化的目的就是让这个特征空间的效果更好。理论上是否有解释？还是就是工程经验？
- P42 页怎么理解，$H$ 就是我们要学习的特征空间（之前表述为 $U$），设其中一个向量为 $u_i$，$u_i^T L u_i=lambda_i u_i^T u_i=lambda_i norm(u_i)$，如果让 $u_i$ 归一化，那么就等于 $lambda_i$，求 $"trace"$ 的最小也就是取最小的 $k$ 个特征值

