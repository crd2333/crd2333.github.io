---
order: 5
---

#import "/src/components/TypstTemplate/lib.typ": *

#show: project.with(
  title: "AI 笔记之强化学习",
  lang: "zh",
)

= Other Technique
== 完全去中心化学习(FDMARL)
- 之前讲的 MA 都是基于 CTDE 的假设，即智能体在学习的过程中可以获取全局信息，但是在执行的过程中是分布式的。但是在一些情况下，智能体在学习和执行的过程中都是分布式的，这就是完全去中心化学习
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-09-45-22.png")
- 思考：CTDE设定对于MARL问题是否可以是默认假设？
  - 其实，目前来看，基本上大部分方法在 CTDE 的假设下才能 work，似乎是一个默认假设；换句话说，目前去中心化学习效果不是很好
    - 有CTDE假设的多智能体强化学习问题可以被视为行为空间指数提升的单智能体强化学习问题。在单智能体环境中，解决连续行为空间的多种方法，对多智能体设定依然有较大提升
  - 类似于一个桥梁，把单智能体的方法应用到多智能体上
  - 但存在一些问题（情景），不存在集中训练的前提。例如司机对超车意图的判定，司机（智能体）在训练过程中无法获取全局的态势
- 完全去中心化学习(Fully Decentralized Learning)
  + 更高的可扩展性
  + 更符合实际设定
  + MARL与Single-Agent RL in Multi-agent Setting的核心区别之一
  - 回忆之前讲的 MARL 范例：联合学习、独立学习、分解学习(CTDE)，FDMARL 更接近第二种
- 困难
  - 每个智能体只通过局部观测、自身行为、获取的奖励，而不通过智能体间的交流或者参数分享来训练策略。
  - 环境不稳定：
    $
    P(s'|s,a) &-> P(s'|s,a_i) = sum_(a_(-i)) pi_(-i) (a_(-i)|s) P(s'|s,a_i, a_(-i))\
    r(s,a) &-> r(s,a_i) = sum_(a_(-i)) pi_(-i) (a_(-i)|s) r(s,a_i, a_(-i))
    $
  - 因为$pi_(-i)$在学习的过程中同时更新，针对每一个智能体环境变得不稳定，因此简单的方法诸如IQL，IPPO等方法没有收敛保证。
- 思路：智能体不同步更新，每次只有一个智能体进行策略更新
  - IQL 没有收敛保证，MA2QL 有
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-09-59-53.png")
- MA2QL：多智能体轮流策略迭代
  - 有收敛保证（纳什均衡），即收敛但未必全局最优
  - 这种轮流迭代的方法无论是基于策略还是基于价值函数，最终都会收敛到纳什均衡
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-10-09-46.png")
  - 轮流更新与单步策略提升需要太久收敛
- FDMARL 的圈子实际上目前比较小，北大有个组提出了一个 I2Q 的算法和一定的理论推导，这里不展开

== 分布式RL
- 首先介绍分布式机器学习
  + 计算量太大：基于共享内存的多线程或多机并行计算
  + 训练数据太多：划分多个节点训练（数据并行）
  + 模型规模太大：将模型划分为多个模块，在不同节点上进行训练（模型并行）；不同子模型一般依赖性较强，对通信的要求较高。
  - 对强化学习来说，训练数据太多基本不会遇到（因为也就只是 $s,a,r,s'$，或者最多 $s$ 以图片等形式输入，一般不太会爆）；模型规模也不会太大，因为基本只是采用比较简单的 CNN or RNN or MLP；主要问题是计算量
#grid(
  columns: 2,
  [
  - 一般分为以下几个模块：
    - 数据与模型划分模块
    - 单机优化模块
    - 通信模块
    - 数据与模型聚合模块
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-10-25-09.png")
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-10-26-01.png")
  ],
  [
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-10-22-08.png")
  - 模型聚合的基本方式（左图）：

    + 参数平均
      - 每个 worker 都有当前模型参数的备份
      - 每个 worker 在各自的子数据集上训练
      - 全局参数被设置为各个 worker 模型参数的平均
    + 分布式随机梯度下降
      - 梯度回传至参数服务器进行更新
    + 去中心化异步随机梯度下降：
      - 基于中心参数服务器的模型聚合方法具有单节点故障问题
      - 不再设置中心参数服务器
      - 通过端到端的通信来传递模型的更新
  ]
)

- 回到分布式机器学习
  - 单机强化学习基本流程：
    - 一般情况下只有单个智能体与单个环境交互获取经验样本，将大量时间用于了数据收集阶段，这也是强化学习的特点之一。
    - 因此想要提高训练效率，一个重点就是需要提高采样效率。
  - 分布式强化学习方法中引入了多个智能体同时与环境交互并行采样，以提升单位时间内所能获得的样本量。
    - 在具体实现时，一般将智能体解耦为两部分：Actor 与 Learner。其中 Actor 负责与环境交互获取数据，而 Learner 则利用收集而来的数据进行批量更新。
- A3C 和 A2C（A3C同步版，不是之前说的 Advantage 版）
- IMPALA 强化学习框架
- 并行PPO(DPPO)
  - 原始PPO算法（单线程）：每一轮优化中，策略串行地收集轨迹，并进行策略更新
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-10-46-48.png")
  - 哪些流程可以并行：
    + 不同轨迹的收集：在收集轨迹的过程中，不同的轨迹都是由同一个当前策略与环境交互得到的，不同轨迹的收集之间相互独立，所以我们可以并行地收集策略。
    + 轨迹收集与策略更新：策略的更新可以不必等待所有轨迹收集完毕，而是在满足一定条件后直接开始更新全局策略
  - 分布式近端策略优化算法(Distributed Proximal Policy Optimization，DPPO)，直接地采用的多 worker 并行地收集轨迹并计算梯度，然后主线程(chief)将个 worker 计算的梯度平均，并回传给 worker 用于更新其策略。流程图：
  #fig("/public/assets/AI/AI_RL/img-2024-07-12-10-48-58.png")
- DPPO 存在的问题：
  - 共享经验池中的轨迹样本由多个不同的策略与环境交互得到，这导致优化的策略和采集样本的策略有很大程度的差异进一步加剧。
  - 当$pi_th >> pi_(th"old")$时，$r_t (th)$将非常巨大。这种情况下如果 $A_t < 0$，计算得到裁剪代理目标为 $L^"CLIP"=max(r_t (th),1-ep)A_t=r_t (th) A_t$ 将会是一个非常大的负数，引入巨大且无界的方差，导致算法无法最终收敛
  - 开悟 DPPO 的解决方法 #mitex(`\mathbb{E}\left[\text{max}(\text{min}(r_{t}(\theta)A_{t},c l i p(r_{t}(\theta),1-\epsilon,1+\epsilon)A_{t}),c A_{t})\right]`)
  - 开悟 DPPO
    + AI Servers：用于与环境交互收集策略。
    + RL Learner：学习器，用于学习策略。多个RL Learner并行地通过共享内存从经验池采集样本用于策略学习
    + Dispatch：中介模块，用于提高收集到的样本传输到共享内存的速度
    + Memory Pool：内存池，用于存储轨迹经验。供多个RL Learner训练。



