---
order: 1
---

#import "/src/components/TypstTemplate/lib.typ": *

#show: project.with(
  title: "",
  lang: "zh",
)

- Reinforcement Learning
- 其他人的博客 or 笔记
  + #link("https://www.cnblogs.com/pinard/category/1254674.html")[刘建平Pinard的强化学习随笔]

#note(caption: "注：")[
  - 个人认为 rl 的本质是把无梯度的 feedback 转换为可训练的梯度
  - 具体使用什么样的框架更好，并不影响 rl 理论的本质
  - 这也许就是为什么感觉现在做 rl 理论的都在叫苦，但 rl 应用却越来越多（比如大模型 ChatGPT-o1 等）
]

#let epg = $epsilon"-greedy"$

= 基本概念
== 动态规划
- 动态规划将复杂的多阶段决策问题分解为一系列简单的、离散的单阶段决策问题，采用顺序求解方法，通过求解一系列小问题达到求解整个问题的目的
- 其各个决策阶段不但要考虑本阶段的决策目标，还要兼顾整个决策过程的整体目标，从而实现整体的最优决策
- 适合于用动态规划的方法求解的问题是具备无后效性（马尔可夫性）的决策过程，更重要的是需要有环境的完整信息
  - 但不幸的是，大多数情况下我们很难直接获取状态转移和奖励函数
  - 换句话说，这一块挺冲击波的，基本没啥用，不用搞太清楚

== 免模型学习
- 强化学习的有模型学习 (Model based learning) 应该也是基于上述动态规划思想
- 与之相对的是更符合实际情况的无模型学习或者叫免模型学习 (Model free learning)
  - 转移概率，奖赏函数未知
  - 甚至环境中的状态数目也未知
  - 假定状态空间有限
- 免模型学习所面临的困难
  - 策略无法评估 $->$ 多次采样
  - 无法通过值函数计算状态-动作值函数 $->$ 直接估计每一对状态-动作的值函数
  - 机器只能从一个起始状态开始探索环境 $->$ 在探索过程中逐渐发现各个状态

== MDP 建模
- 马尔可夫决策过程 (Markov decision process, MDP)
  - 几乎所有的强化学习问题可以用马尔可夫决策过程 (MDPs) 来描述
  - 所谓马尔可夫，就是指：在给定当前状态的条件下，未来与过去条件独立
    - 也就是当前状态包含了决定未来所需的所有信息（未来决定于现在，与过去无关）
  - 更正式地说，马尔可夫过程由 $5$ 个元素组成 $M = <S, A, P, R, ga>$
    + $S$: 状态集合（空间）
    + $A$: 动作集合（空间）
    + $P$: 状态转移函数
    + $R$: 奖励函数
    + $ga$: 折扣因子（未来奖励衰减系数），一些文献中不将其视为 MDP 的一部分
    - 在未知环境中，转移概率 $P$ 和奖励 $R$ 由环境给出，无法获取其全部信息
  - 部分可观测马尔科夫决策过程 (Partially Observable Markov Decision Process, POMDP)
    - 是带有隐藏状态的 MDP，是一个带有行为决策的隐马尔科夫模型 $M = <S, A, O, P, R, Z, ga>$
      + $S$: 有限状态集合（空间）
      + $A$: 有限动作集合（空间）
      + $O$: 有限观测集合（空间）
      + $P$: 状态转移函数，$P_(s s')^a = PP[S_(t+1)=s'|S_t=s,A_t=a]$
      + $R$: 奖励函数，$R_s^a = PP[R_(t+1)|S_t=s,A_t=a]$
      + $Z$: 基于状态 $S$ 的观测函数 $Z_(s',o)^a = PP[O_(t+1)=o+S_(t+1)=s',A_t=a]$
      + $ga$: 折扣因子（未来奖励衰减系数）
- 贝尔曼方程
  - 指一系列等式，将价值函数和动作价值函数分解为当前直接奖励加上后续衰减后的未来奖励
- 强化学习 v.s. 监督学习
  - 监督学习：给有标记样本，用分类器、回归器预测标签
  - 强化学习：没有标记样本，通过与环境交互，利用执行动作之后反馈的奖赏学习最优策略
  - 强化学习在某种意义上可以认为是具有 “延迟标记信息” 的监督学习

== 策略评估与优化
- 强化学习由策略评估和策略改进组成
  - 策略迭代算法 = 随机策略作为初始策略，随后循环进行 策略评估+策略提升，直到策略收敛
  - 策略迭代算法在每次改进策略后都需要重新进行策略评估
- 策略评估的目标是知道什么决策是好的，即估计 $V^pi (s_t)$
- 策略提升的目标是根据值函数选择好的行动
  - 基于价值函数 $V(s)$
    $ pi(s) = argmax_(a in A) sum_(s' in S) underbrace(P(s'|s,a) V(s'), "需要知道环境模型") $
  - 基于动作价值函数 $Q(s,a)$
    $ pi(s) = argmax_(a in A) underbrace(Q(s,a), "直接作用于决策") $
- 根据策略评估和提升时所用的策略是否是同一策略，可以分为同策略 (on-policy) 和异策略 (off-policy) 方法
  - 同策略：评估和提升的策略相同
  - 异策略：评估和提升的策略不同

== 探索与利用
- 为什么？环境信息不完全；每一种决策的真实价值无法获取，只能获取其统计价值。
  - 更具体地，有些行为尚未被探索过，其价值未知；已被探索过的行为可能因为偶然概率原因被高估或者低估；
=== $epg$ 方法
- 大多数时间 ($1 - epsilon$) 采用当前统计行为价值中最优的行为，其余时间随机选择一个行为 ($epsilon$)
  - 每个行为的价值为历史决策经验中该行为获得奖励的均值
  #mitex(`\hat{Q}_{t}(a)=\frac{1}{N_{t}(a)}\sum_{\tau=1}^{t}r_{\tau}\,\delta[a_{\tau}=a]`)其中 $delta$ 为二元指示函数，$N_t (a)$ 为 $t$ 步中行为 $a$ 被选择的次数，即 $display(N_t (a) = sum_(tau=1)^t delta[a_tau=a])$
  - $ep$ 的选择
    - 可以被设定为超参，也可以随训练的进行逐步衰减，例如 $ep=1/sqrt(t)$
    - 如果任务不确定性较大，概率分布较宽，可以增大 $ep$
  - 优点：简单，易于理解
  - 缺点：虽然每个行为都有可能是最优行为，但是其成为最优的可能性是不同的，同理选择具备不同潜力的行为的策略也可以不完全随机 $->$ UCB 方法

=== Softmax 方法
- 基于当前已知的平均奖赏来对探索与利用折中
  - 以摇臂赌博机为例，若某个摇臂当前的平均奖赏越大，则它被选择的概率越高
  - 概率分配使用 Boltzmann 分布：
    $ P(k) = frac(e^(Q(k)/ta), sumiK e^(Q(i)/ta)) $
    - $Q(i)$ 为当前摇臂的平均奖赏
    - $ta > 0$ 为温度。$ta -> 0$ 仅利用；$ta -> infty$ 仅探索
- $epg$ 与 Softmax 算法比较
  - 孰优孰劣取决于具体应用问题（西瓜书这话说了跟没说一样 #emoji.face.sweat）
  - 两种算法都有折中参数 ($ep, ta$)

=== UCB 方法
- 通过奖励值的上置信界 (Upper Confidence Bound) 来衡量每一个动作附加其“潜力”后的价值。
  - 行为的真实价值低于附加潜力后的价值，即：$Q(a) =< hat(Q)_t (a) + hat(U)_t (a)$
  - 上界函数 $U_t (a)$ 跟 $N_t (a)$ 相关，因为大的行为访问次数使得对应的行为的价值更准确，因此得到较小的置信上界。
  - 首先介绍 Hoeffding 不等式：对于独立同分布的随机变量 $X_1, X_2, ..., X_n$，，$t$ 次采样得到的样本均值为 $macron(X)_t$，那么对于给定的 $u$：$ P(E(X) > macron(X)+u) <= e^(-2 t u^2) $
  - 替换得到我们的结论 #mitex(`\mathbb{P}[Q(a)>{\hat{Q}}_{t}(a)+U_{t}(a)]\leq e^{-2t U_{t}(a)^{2}}`)
  - 令上式右端为 $p$，反解得到 $U_t (a) = sqrt((-ln p)/(2 N_t (a)))$
  - 一种启发方法是及时降低 $p$ 的阈值，因此常设其为 $t$ 的负数次方，得到 $U_t (a) = sqrt((c ln t)/(N_t (a)))$
  - 当 $c=2$（$c$越大，越趋向探索），UCB1 算法选择这样的动作 #mitex(`a_{t}^{U C B1}=\arg\operatorname*{max}_{a\in A}\left(Q(a)+{\sqrt{{\frac{2\log t}{N_{t}(a)}}}}\right)`)

=== 熵正则
- 我们希望策略网络的输出的概率不要集中在一个动作上，至少要给其他的动作一些非零的概率，让这些动作能被探索到。可以用熵 (Entropy) 来衡量概率分布的不确定性